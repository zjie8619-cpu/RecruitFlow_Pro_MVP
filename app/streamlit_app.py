import json
import os
import re
import time

import pandas as pd
import streamlit as st
from datetime import datetime
from pathlib import Path
from backend.storage.db import init_db, get_db
from backend.services.pipeline import RecruitPipeline
from backend.services.reporting import export_round_report
from backend.utils.versioning import VersionManager
from backend.utils.field_mapping import translate_dataframe_columns, translate_field
from backend.services.jd_ai import generate_jd_bundle
from backend.services.resume_parser import parse_uploaded_files_to_df
from backend.services.ai_matcher import ai_match_resumes_df
from backend.services.ai_core import generate_ai_summary, generate_ai_email
from backend.services.calendar_utils import create_ics_file
from backend.services.excel_exporter import generate_competency_excel
from dotenv import load_dotenv

# å°è¯•ä»å¤šä¸ªä½ç½®åŠ è½½.envæ–‡ä»¶
env_paths = [
    Path('.env'),  # å½“å‰ç›®å½•ï¼ˆapp/ï¼‰
    Path('../.env'),  # é¡¹ç›®æ ¹ç›®å½•
    Path(__file__).parent.parent / '.env',  # é¡¹ç›®æ ¹ç›®å½•ï¼ˆç»å¯¹è·¯å¾„ï¼‰
]
for env_path in env_paths:
    if env_path.exists():
        load_dotenv(dotenv_path=env_path)
        break
else:
    load_dotenv()  # é»˜è®¤åŠ è½½

# --- session åˆå§‹åŒ–ï¼ˆæ”¾åœ¨ import ä¹‹åï¼‰---
if "ai_bundle" not in st.session_state:
    st.session_state["ai_bundle"] = None

# ============ æ§åˆ¶æ˜¾ç¤ºéƒ¨åˆ† ============
SHOW_OFFLINE_SECTION = False   # æ˜¯å¦æ˜¾ç¤ºâ€œç¦»çº¿è§„åˆ™ç‰ˆâ€
SHOW_DETAIL_SECTIONS = True   # æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†éƒ¨åˆ†ï¼ˆé•¿ç‰ˆJD / å²—ä½èƒ½åŠ›ç»´åº¦ / é¢è¯•é¢˜ç­‰ï¼‰
# =====================================

st.set_page_config(page_title="RecruitFlow | ä¸€é”®æ‹›è˜æµæ°´çº¿", layout="wide")
st.title("RecruitFlow â€” ä¸€é”®æ‹›è˜æµæ°´çº¿ï¼ˆæ•™è‚²æœºæ„ç‰ˆï¼‰")

with st.sidebar:
    st.header("è®¾ç½®")
    cfg_file = Path("backend/configs/model_config.json")
    cfg = json.loads(cfg_file.read_text(encoding="utf-8"))
    
    # AIé…ç½®ï¼ˆé”å®šä¸ºGPT-4ï¼‰
    st.subheader("AIé…ç½®")
    st.markdown("**AIæä¾›å•†ï¼š** OpenAI (å·²é”å®š)")
    st.markdown("**æ¨¡å‹åç§°ï¼š** GPT-4 (å·²é”å®š)")
    st.info("ğŸ”’ AIé…ç½®å·²é”å®šä¸ºGPT-4ï¼Œç¡®ä¿ç”Ÿæˆè´¨é‡ã€‚å¦‚éœ€ä¿®æ”¹ï¼Œè¯·è”ç³»ç®¡ç†å‘˜ã€‚")
    st.caption("ğŸ’¡ è¯·è®¾ç½®ç¯å¢ƒå˜é‡: OPENAI_API_KEY æˆ–é…ç½® backend/configs/api_keys.json")
    
    # å›ºå®šä½¿ç”¨GPT-4
    llm_provider = "openai"
    llm_model = "gpt-4"
    
    st.markdown("---")
    
    # å…¶ä»–è®¾ç½®
    st.subheader("ç­›é€‰è®¾ç½®")
    
    blind = st.toggle("ç›²ç­›æ¨¡å¼ï¼ˆéšè—å§“å/å­¦æ ¡ç­‰ï¼‰", value=cfg.get("blind_screen", True),
                     help="å¼€å¯åï¼Œåœ¨ç®€å†ç­›é€‰è¿‡ç¨‹ä¸­éšè—å€™é€‰äººçš„å§“åã€å­¦æ ¡ç­‰æ•æ„Ÿä¿¡æ¯ï¼Œé¿å…å› ä¸ªäººèƒŒæ™¯äº§ç”Ÿåè§ï¼Œç¡®ä¿å…¬å¹³ç­›é€‰")
    
    thr = st.slider("ç½®ä¿¡åº¦é˜ˆå€¼", 0.0, 1.0, cfg.get("confidence_threshold", 0.65), 0.05,
                    help="è¯„åˆ†ç½®ä¿¡åº¦ä½äºæ­¤é˜ˆå€¼çš„å€™é€‰äººå°†è¢«æ ‡è®°ä¸º'é˜ˆå€¼æ‹¦æˆª'ï¼Œä¸ä¼šè‡ªåŠ¨å‘é€é‚€çº¦ã€‚å»ºè®®å€¼ï¼š0.6-0.7ã€‚å€¼è¶Šé«˜ï¼Œç­›é€‰è¶Šä¸¥æ ¼ã€‚")
    
    st.caption("ğŸ’¡ ç½®ä¿¡åº¦é˜ˆå€¼è¯´æ˜ï¼šç³»ç»Ÿä¼šæ ¹æ®ç®€å†åŒ¹é…åº¦è®¡ç®—ä¸€ä¸ªç½®ä¿¡åº¦åˆ†æ•°ã€‚ä½äºé˜ˆå€¼çš„å€™é€‰äººéœ€è¦äººå·¥å®¡æ ¸åæ‰èƒ½é‚€çº¦ã€‚")
    if st.button("ä¿å­˜è®¾ç½®"):
        cfg["blind_screen"] = blind
        cfg["confidence_threshold"] = float(thr)
        # AIé…ç½®å·²é”å®šï¼Œä¸æ›´æ–°
        cfg["llm_provider"] = "openai"
        cfg["llm_model"] = "gpt-4"
        cfg_file.write_text(json.dumps(cfg, ensure_ascii=False, indent=2), encoding="utf-8")
        st.success("âœ… è®¾ç½®å·²ä¿å­˜ï¼ˆAIé…ç½®ä¿æŒé”å®šä¸ºGPT-4ï¼‰")
    st.markdown("---"); st.caption("ç‰ˆæœ¬æ§åˆ¶")
    vm = VersionManager()
    if st.button("åˆ›å»ºå¿«ç…§"):
        st.success("å·²åˆ›å»ºç‰ˆæœ¬ï¼š" + vm.snapshot())


init_db(); pipe = RecruitPipeline()
tab1, tab2, tab3, tab4, tab5 = st.tabs(["1 ç”Ÿæˆ JD","2 ç®€å†è§£æ & åŒ¹é…","3 å»é‡ & æ’åº","4 é‚€çº¦ & æ’æœŸ","5 é¢è¯•åŒ… & å¯¼å‡º"])

with tab1:
    # ==========================================================
    # âœ… ç»Ÿä¸€æŒ‰é’®å®šä¹‰ï¼Œé˜²æ­¢ StreamlitDuplicateElementId é”™è¯¯
    # ==========================================================
    
    # ğŸ”¹ åŠŸèƒ½ï¼šä¿å­˜ JD ä¸ Rubric æ•°æ®
    # ğŸ”¹ ä¿®å¤ï¼šä¸ºæŒ‰é’®åˆ†é…å”¯ä¸€ keyï¼Œé˜²æ­¢é‡å¤ ID å†²çª
    # ğŸ”¹ æµ‹è¯•ç»“æœï¼šå·²é€šè¿‡å¤šæ¬¡ Cursor ä¸ Streamlit è¿è¡ŒéªŒè¯ï¼ˆæ— å¼‚å¸¸ï¼‰
    # ==========================================================
    
    # å°è£…æŒ‰é’®è¡Œä¸ºï¼ˆå¯å¤ç”¨ï¼‰
    def save_to_system_action():
        """ç»Ÿä¸€çš„ä¿å­˜ JD + é¢˜åº“æ“ä½œ"""
        current_bundle = st.session_state.get("ai_bundle")
        if not current_bundle:
            st.warning('è¯·å…ˆç‚¹å‡»"ç”Ÿæˆ JD"è·å¾— AI ç»“æœã€‚')
            return
        
        try:
            job_to_save = (st.session_state.get("job_name") or "").strip()
            if not job_to_save:
                job_to_save = current_bundle.get("rubric", {}).get("job", "")
    
            pipe.save_jd(job_to_save, current_bundle["jd_long"], current_bundle["jd_short"], current_bundle["rubric"])
    
            q_path = Path("data/templates/é¢˜åº“ç¤ºä¾‹.csv")
            rows = []
            for q in current_bundle.get("interview", []):
                points = q.get("points") or []
                points_str = "ï¼›".join(points) if isinstance(points, list) else (str(q.get("points", "")) if q.get("points") else "")
                rows.append({
                    "job": job_to_save,
                    "èƒ½åŠ›ç»´åº¦": q.get("dimension", "é€šç”¨"),
                    "é¢˜ç›®": q.get("question", ""),
                    "è¯„åˆ†è¦ç‚¹": points_str,
                    "åˆ†å€¼": int(q.get("score", 0)),
                    "æƒé‡": round(float(q.get("score", 0)) / 100.0, 4)
                })
            if rows:
                qdf = pd.DataFrame(rows)
                q_path.parent.mkdir(parents=True, exist_ok=True)
                header = not q_path.exists()
                qdf.to_csv(q_path, mode="a", index=False, encoding="utf-8-sig", header=header)
            st.success("å·²å†™å…¥ï¼šJD / Rubric / é¢˜åº“")
        except Exception as e:
            st.error(f"âŒ å†™å…¥å¤±è´¥ï¼š{e}")
    
    st.subheader("æ™ºèƒ½ç”Ÿæˆ JDï¼ˆAIåˆ†æï¼‰")
    
    # === æ–°å¢ï¼šæ™ºèƒ½ç”Ÿæˆ JDï¼ˆAI åˆ†æï¼‰ ===
    st.markdown("### ğŸ¤– æ™ºèƒ½ç”Ÿæˆ JDï¼ˆAI åˆ†æï¼‰")
    
    # é¢„æ£€æŸ¥ï¼šAI Key
    key_present = bool(os.getenv("SILICONFLOW_API_KEY") or os.getenv("OPENAI_API_KEY"))
    if not key_present:
        st.warning("âš ï¸ æœªæ£€æµ‹åˆ° AI Keyï¼šè¯·åœ¨é¡¹ç›®æ ¹ç›®å½•åˆ›å»º `.env` å¹¶é…ç½® SILICONFLOW_API_KEYã€‚")
    
    with st.form("ai_jd_form"):
        ai_job = st.text_input("å²—ä½åç§° *", value=st.session_state.get("job_name",""), help="ä¾‹å¦‚ï¼šæ•°å­¦ç«èµ›æ•™ç»ƒ/æ•™å­¦è¿è¥ä¸“å‘˜/ç­ä¸»ä»»/Javaåç«¯")
        ai_must = st.text_area("å¿…å¤‡ç»éªŒ/æŠ€èƒ½", value="", height=80, help="åˆ†å·æˆ–ç©ºæ ¼åˆ†éš”ï¼Œä¾‹å¦‚ï¼šå›½ä¸€; LaTeX; IMOè®­ç»ƒ")
        ai_nice = st.text_area("åŠ åˆ†é¡¹", value="", height=60, help="å¦‚ï¼šç«èµ›å‡ºé¢˜ç»éªŒ; å…¬å¼€è¯¾; å†…å®¹åˆ¶ä½œ")
        ai_excl = st.text_area("æ’é™¤é¡¹", value="", height=60, help="å¦‚ï¼šä»…å®ä¹ ; å…¼èŒ")
        submitted = st.form_submit_button("ğŸš€ ç”Ÿæˆ JD", type="primary", use_container_width=True)
        
        if submitted:
            if not ai_job:
                st.error("âŒ è¯·å¡«å†™å²—ä½åç§°")
            else:
                st.session_state["job_name"] = ai_job
                # è¾“å…¥æ¸…æ´—ï¼štex -> LaTeX
                ai_must = ai_must.replace("tex", "LaTeX").replace("Tex", "LaTeX")
                ai_nice = ai_nice.replace("tex", "LaTeX").replace("Tex", "LaTeX")
                try:
                    with st.spinner("ğŸ¤– AIæ­£åœ¨æ™ºèƒ½åˆ†æå²—ä½éœ€æ±‚ï¼Œç”Ÿæˆä¸“ä¸šJDã€èƒ½åŠ›ç»´åº¦ã€é¢è¯•é¢˜ç›®ï¼Œè¯·ç¨å€™ï¼ˆé€šå¸¸éœ€è¦10-30ç§’ï¼‰..."):
                        bundle = generate_jd_bundle(ai_job, ai_must, ai_nice, ai_excl)
                        # åŸºäºé•¿ç‰ˆ JD å†åšä¸€æ¬¡â€œçŸ­ç‰ˆJDæå– + ä»»èŒè¦æ±‚æŠ½å–èƒ½åŠ›ä¸é¢è¯•é¢˜â€
                        from backend.services.jd_ai import extract_short_and_competencies_from_long_jd
                        extracted = extract_short_and_competencies_from_long_jd(bundle.get("jd_long",""), ai_job)
                        if extracted:
                            # ç”¨æŠ½å–å¾—åˆ°çš„çŸ­ç‰ˆ JD è¦†ç›–
                            if extracted.get("short_jd"):
                                bundle["jd_short"] = extracted["short_jd"]
                            # ç”¨æŠ½å–å¾—åˆ°çš„èƒ½åŠ›ç»´åº¦/é¢è¯•é¢˜è¦†ç›–å±•ç¤ºï¼ˆè½¬æ¢ä¸ºå†…éƒ¨æ ¼å¼ï¼‰
                            dims = []
                            for d in extracted.get("èƒ½åŠ›ç»´åº¦", []):
                                anchors = d.get("è¯„åˆ†é”šç‚¹") or {}
                                dims.append({
                                    "name": d.get("ç»´åº¦åç§°", ""),
                                    "weight": round(float(d.get("æƒé‡", 0)) / 100.0, 4),
                                    "desc": d.get("å®šä¹‰", ""),
                                    "anchors": {
                                        "20": anchors.get("20") or "åŸºç¡€è¾¾æˆï¼šè¯·ç»“åˆ JD ä¸­çš„åŸºç¡€è¦æ±‚æè¿°ã€‚",
                                        "60": anchors.get("60") or "è‰¯å¥½è¾¾æˆï¼šèƒ½å¤Ÿç¨³å®šäº§å‡ºå¹¶ä¸æ–­ä¼˜åŒ–ã€‚",
                                        "100": anchors.get("100") or "ä¼˜ç§€è¾¾æˆï¼šæŒç»­è¾“å‡ºæ°å‡ºæˆæœå¹¶é‡åŒ–å½±å“ã€‚",
                                    },
                                })
                            if dims:
                                bundle["dimensions"] = dims
                            qs = []
                            for q in extracted.get("èƒ½åŠ›ç»´åº¦_é¢è¯•é¢˜", []):
                                raw_points = q.get("è¯„åˆ†è¦ç‚¹", [])
                                if isinstance(raw_points, str):
                                    points_list = [p.strip() for p in re.split(r"[ï¼›;ã€\n]", raw_points) if p.strip()]
                                else:
                                    points_list = [str(p).strip() for p in (raw_points or []) if str(p).strip()]
                                question_text = q.get("é¢è¯•é¢˜", "")
                                if isinstance(question_text, list):
                                    question_text = "ï¼›".join(str(item).strip() for item in question_text if str(item).strip())
                                qs.append({
                                    "dimension": q.get("ç»´åº¦åç§°", ""),
                                    "question": question_text,
                                    "points": points_list,
                                    "score": float(q.get("åˆ†å€¼", 0)),
                                })
                            if qs:
                                bundle["interview"] = qs
                    # âœ… æŒä¹…åŒ–ï¼šåç»­å…¶å®ƒæŒ‰é’®/åŒºåŸŸå¯å¤ç”¨
                    st.session_state["ai_bundle"] = bundle
                    st.success("âœ… AI ç”Ÿæˆå®Œæˆ")
                except Exception as e:
                    error_msg = str(e)
                    # æå–æ›´å‹å¥½çš„é”™è¯¯ä¿¡æ¯
                    if "Key" in error_msg or "æœªé…ç½®" in error_msg:
                        st.error(f"âŒ {error_msg}")
                        st.info("ğŸ’¡ è¯·æ£€æŸ¥é¡¹ç›®æ ¹ç›®å½•çš„ `.env` æ–‡ä»¶ï¼Œç¡®ä¿åŒ…å« SILICONFLOW_API_KEY æˆ– OPENAI_API_KEYï¼Œç„¶åé‡å¯ Streamlitã€‚")
                    elif "401" in error_msg or "403" in error_msg:
                        st.error(f"âŒ API Key éªŒè¯å¤±è´¥ï¼š{error_msg}")
                        st.info("ğŸ’¡ è¯·æ£€æŸ¥ .env æ–‡ä»¶ä¸­çš„ API Key æ˜¯å¦æ­£ç¡®ï¼Œæˆ–æ˜¯å¦å·²è¿‡æœŸã€‚")
                    elif "404" in error_msg or "æ¨¡å‹" in error_msg:
                        st.error(f"âŒ æ¨¡å‹ä¸å¯ç”¨ï¼š{error_msg}")
                        st.info("ğŸ’¡ è¯·æ£€æŸ¥ .env æ–‡ä»¶ä¸­çš„ AI_MODEL æ˜¯å¦æ­£ç¡®ï¼Œæˆ–å°è¯•æ›´æ¢ä¸ºå…¶ä»–å¯ç”¨æ¨¡å‹ï¼ˆå¦‚ Qwen2.5-32B-Instructï¼‰ã€‚")
                    else:
                        st.error(f"âŒ AI ç”Ÿæˆå¤±è´¥ï¼š{error_msg}")
                        st.info("ğŸ’¡ ç³»ç»Ÿå°†ç»§ç»­æ”¯æŒ'ç¦»çº¿è§„åˆ™ç‰ˆ'ç”Ÿæˆï¼Œç¡®ä¿å¯ç”¨ã€‚å±•å¼€ä¸‹æ–¹çš„'AI è¿æ¥è¯Šæ–­'æŸ¥çœ‹è¯¦ç»†é”™è¯¯ä¿¡æ¯ã€‚")
    
    # æ˜¾ç¤ºAIç”Ÿæˆç»“æœ
    bundle = st.session_state.get("ai_bundle")
    if SHOW_DETAIL_SECTIONS:
        if bundle:
            st.subheader("ğŸ“„ é•¿ç‰ˆ JDï¼ˆBossç›´è˜å¯ç”¨ï¼‰")
            st.text_area("é•¿ç‰ˆ JD", bundle["jd_long"], height=260)
        
            st.subheader("ğŸª§ çŸ­ç‰ˆ JDï¼ˆç¤¾åª’/å†…æ¨ï¼‰")
            st.text_area("çŸ­ç‰ˆ JD", bundle["jd_short"], height=100)
        
            # 1ï¸âƒ£ ç”Ÿæˆå²—ä½èƒ½åŠ›ç»´åº¦è¡¨ df_dimensionsï¼ˆå«åˆ†å€¼è®¡ç®—é€»è¾‘ï¼‰
            st.subheader("ğŸ¯ å²—ä½èƒ½åŠ›ç»´åº¦ï¼ˆAI åˆ†æï¼‰")
            question_map = {q.get("dimension"): q for q in bundle.get("interview", [])}
            competency_rows = []
            for dim in bundle["dimensions"]:
                anchors = dim.get("anchors") or {}
                question_entry = question_map.get(dim.get("name")) or {}
                question_text = question_entry.get("question")
                if isinstance(question_text, list):
                    question_text = "\n".join(str(item).strip() for item in question_text if str(item).strip())
                question_text = question_text or ""
                points_data = question_entry.get("points") or []
                if isinstance(points_data, str):
                    points_text = "\n".join(p.strip() for p in re.split(r"[ï¼›;ã€\n]", points_data) if p.strip())
                else:
                    points_text = "\n".join(str(p).strip() for p in points_data if str(p).strip())
                competency_rows.append({
                    "èƒ½åŠ›ç»´åº¦": dim.get("name", ""),
                    "è¯´æ˜": dim.get("desc", ""),
                    "æƒé‡(%)": round(float(dim.get("weight", 0)) * 100, 1),
                    "é¢è¯•é—®é¢˜": question_text,
                    "è¯„åˆ†è¦ç‚¹": points_text,
                    "20åˆ†è¡Œä¸ºè¡¨ç°": anchors.get("20", ""),
                    "60åˆ†è¡Œä¸ºè¡¨ç°": anchors.get("60", ""),
                    "100åˆ†è¡Œä¸ºè¡¨ç°": anchors.get("100", ""),
                })

            df_dimensions = pd.DataFrame(competency_rows)
            st.dataframe(df_dimensions, use_container_width=True)

            # å¯¼å‡º Excel
            excel_bytes = generate_competency_excel(bundle["dimensions"], bundle.get("interview", []))
            download_name = f"{(st.session_state.get('job_name') or 'å²—ä½').strip()}_èƒ½åŠ›ç»´åº¦è¯„åˆ†è¡¨.xlsx"
            st.download_button(
                "ğŸ“„ å¯¼å‡ºèƒ½åŠ›ç»´åº¦è¯„åˆ†è¡¨ï¼ˆExcelï¼‰",
                data=excel_bytes,
                file_name=download_name,
                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                use_container_width=True,
            )

            with st.expander("ğŸ” è¯„åˆ†é”šç‚¹ï¼ˆ20 / 60 / 100 åˆ†è¡Œä¸ºç¤ºä¾‹ï¼‰"):
                for d in bundle["dimensions"]:
                    anchors = d.get("anchors") or {}
                    st.markdown(f"**{d['name']}**")
                    st.markdown(f"- **20 åˆ†**ï¼š{anchors.get('20', 'ï¼ˆæœªæä¾›ï¼‰')}")
                    st.markdown(f"- **60 åˆ†**ï¼š{anchors.get('60', 'ï¼ˆæœªæä¾›ï¼‰')}")
                    st.markdown(f"- **100 åˆ†**ï¼š{anchors.get('100', 'ï¼ˆæœªæä¾›ï¼‰')}")
                    st.markdown("---")
        
            # ------------------ é»˜è®¤ç”Ÿæˆå‡½æ•°ï¼ˆä¿®å¤ImportErrorç”¨ï¼‰ ------------------
            def generate_default_question(dimension_name: str):
                """AI æ— è¿”å›æ—¶çš„é»˜è®¤é¢˜ç›®æ¨¡æ¿"""
                default_questions = {
                    "æ²Ÿé€šè¡¨è¾¾/åŒç†å¿ƒ": "è¯·ä¸¾ä¾‹è¯´æ˜ä½ åœ¨ä¸åŒäº‹æˆ–å®¢æˆ·æ²Ÿé€šä¸­ï¼Œå¦‚ä½•ç†è§£å¹¶å›åº”ä»–äººæƒ…ç»ªä¸éœ€æ±‚ã€‚",
                    "æ‰§è¡ŒåŠ›/ä¸»äººç¿ç²¾ç¥": "è¯·æè¿°ä¸€æ¬¡ä½ é¢å¯¹å·¥ä½œæŒ‘æˆ˜æ—¶ä¸»åŠ¨æ‰¿æ‹…è´£ä»»å¹¶æ¨åŠ¨ä»»åŠ¡å®Œæˆçš„ç»å†ã€‚"
                }
                return default_questions.get(dimension_name, f"è¯·ç»“åˆ{dimension_name}ç»´åº¦ï¼Œæè¿°ä¸€ä¸ªç›¸å…³çš„å…¸å‹å·¥ä½œåœºæ™¯ã€‚")

            def generate_default_rubric(dimension_name: str):
                """AI æ— è¿”å›æ—¶çš„é»˜è®¤è¯„åˆ†è¦ç‚¹"""
                default_rubrics = {
                    "æ²Ÿé€šè¡¨è¾¾/åŒç†å¿ƒ": ["è¡¨è¾¾æ¸…æ™°ï¼›å€¾å¬ä»–äººï¼›å…±æƒ…å›åº”ï¼›è§£å†³å†²çªèƒ½åŠ›å¼ºã€‚"],
                    "æ‰§è¡ŒåŠ›/ä¸»äººç¿ç²¾ç¥": ["è´£ä»»å¿ƒå¼ºï¼›ç§¯æä¸»åŠ¨ï¼›æ‰§è¡Œé«˜æ•ˆï¼›èƒ½å¸¦åŠ¨å›¢é˜Ÿå®Œæˆç›®æ ‡ã€‚"]
                }
                return default_rubrics.get(dimension_name, ["å›ç­”é€»è¾‘æ¸…æ™°ï¼›æœ‰å®é™…æ¡ˆä¾‹ï¼›ä½“ç°æ ¸å¿ƒèƒ½åŠ›ã€‚"])
            # -------------------------------------------------------------------------
        
            # 3ï¸âƒ£ ç”Ÿæˆå²—ä½èƒ½åŠ›ç»´åº¦ä¸é¢è¯•é¢˜è¡¨ df_finalï¼ˆæ¥è‡ª AI åˆ†æ + AI ç”Ÿæˆï¼‰
            interview_list = bundle.get("interview", [])
            
            # æ„å»ºç»´åº¦åç§°åˆ°é¢è¯•é¢˜çš„æ˜ å°„ï¼ˆæŒ‰ç»´åº¦åç§°åŒ¹é…ï¼Œæ›´å¯é ï¼‰
            interview_map = {}
            for q in interview_list:
                dim_name = q.get("dimension", "").strip()
                if dim_name:
                    interview_map[dim_name] = q
            
            # æ„å»ºå¯¹é½è¡¨æ ¼ï¼šå°†ç»´åº¦ä¸é¢è¯•é¢˜ä¸€ä¸€å¯¹åº”ï¼ˆæŒ‰ç»´åº¦åç§°åŒ¹é…ï¼‰
            final_rows = []
            for idx, dim_row in df_dimensions.iterrows():
                dim_name = dim_row["èƒ½åŠ›ç»´åº¦"]
                dim_desc = dim_row["è¯´æ˜"]
                dim_weight = dim_row["æƒé‡(%)"]
                
                # æŒ‰ç»´åº¦åç§°åŒ¹é…å¯¹åº”çš„é¢è¯•é¢˜
                matched_interview = interview_map.get(dim_name)
                
                if matched_interview:
                    points = matched_interview.get("points") or []
                    points_str = "ï¼›".join(points) if isinstance(points, list) else (str(matched_interview.get("points", "")) if matched_interview.get("points") else "")
                    question_text = str(matched_interview.get("question", "")).strip()
                    
                    # ğŸ”§ ä¿®æ­£é€»è¾‘ï¼šå¦‚æœ AI æ²¡è¿”å›å†…å®¹ï¼Œé‡æ–°ç”ŸæˆçœŸå®æ–‡æœ¬è€Œéæç¤ºè¯­
                    if not question_text or question_text == "ï¼ˆå¾…ç”Ÿæˆï¼‰":
                        question_text = generate_default_question(dim_name)
                    
                    # ğŸ”§ ä¿®æ­£é€»è¾‘ï¼šå¦‚æœè¯„åˆ†è¦ç‚¹ä¸ºç©ºï¼Œç”ŸæˆçœŸå®è¯„åˆ†è¦ç‚¹è€Œéæç¤ºè¯­
                    if not points_str or points_str.strip() == "":
                        default_points = generate_default_rubric(dim_name)
                        points_str = "ï¼›".join(default_points) if isinstance(default_points, list) else str(default_points)
                    
                    final_rows.append({
                        "èƒ½åŠ›ç»´åº¦": dim_name,
                        "è¯´æ˜": dim_desc,
                        "æƒé‡(%)": dim_weight,
                        "é¢è¯•é¢˜ç›®": question_text,
                        "è¯„åˆ†è¦ç‚¹": points_str,
                        "åˆ†å€¼": matched_interview.get("score", 0)
                    })
                else:
                    # ğŸ”§ å¦‚æœæ²¡æœ‰å¯¹åº”çš„é¢è¯•é¢˜ï¼Œç”ŸæˆçœŸå®é»˜è®¤å†…å®¹ï¼ˆè€Œéæç¤ºè¯­ï¼‰
                    default_question = generate_default_question(dim_name)
                    default_points_list = generate_default_rubric(dim_name)
                    default_points_str = "ï¼›".join(default_points_list) if isinstance(default_points_list, list) else str(default_points_list)
                    final_rows.append({
                        "èƒ½åŠ›ç»´åº¦": dim_name,
                        "è¯´æ˜": dim_desc,
                        "æƒé‡(%)": dim_weight,
                        "é¢è¯•é¢˜ç›®": default_question,
                        "è¯„åˆ†è¦ç‚¹": default_points_str,
                        "åˆ†å€¼": 0
                    })
            
            df_final = pd.DataFrame(final_rows)
            
            # âœ… åœ¨æ˜¾ç¤ºå‰åŠ è¿™æ®µï¼šåˆ†å€¼ä¸æƒé‡å¯¹é½ä¿®æ­£
            if "æƒé‡(%)" in df_final.columns:
                total_weight = df_final["æƒé‡(%)"].sum()
                df_final["åˆ†å€¼"] = df_final["æƒé‡(%)"].apply(lambda w: round(w * 100 / total_weight, 1))
                total_score = round(df_final["åˆ†å€¼"].sum(), 1)
                if abs(total_score - 100) > 0.1:
                    df_final["åˆ†å€¼"] = df_final["åˆ†å€¼"] * 100 / total_score
                    df_final["åˆ†å€¼"] = df_final["åˆ†å€¼"].round(1)
            
            st.subheader("å²—ä½èƒ½åŠ›ç»´åº¦ä¸é¢è¯•é¢˜ç›®ï¼ˆAIåˆ†æ + AIç”Ÿæˆï¼‰")
            st.dataframe(df_final, use_container_width=True, hide_index=True)
            
            # æ£€æŸ¥æ˜¯å¦æœ‰ç¼ºå¤±é¡¹ï¼ˆåŒ…å«"ï¼ˆå¾…ç”Ÿæˆï¼‰"æˆ–ç©ºå†…å®¹ï¼‰
            has_missing = False
            for _, row in df_final.iterrows():
                if "ï¼ˆå¾…ç”Ÿæˆï¼‰" in str(row.get("é¢è¯•é¢˜ç›®", "")) or not str(row.get("é¢è¯•é¢˜ç›®", "")).strip():
                    has_missing = True
                    break
                if not str(row.get("è¯„åˆ†è¦ç‚¹", "")).strip():
                    has_missing = True
                    break
            
            if has_missing:
                st.warning("âš ï¸ æ£€æµ‹åˆ°éƒ¨åˆ†ç»´åº¦ç¼ºå°‘é¢è¯•é¢˜æˆ–è¯„åˆ†è¦ç‚¹ï¼Œè¯·ç‚¹å‡»ä¸‹æ–¹æŒ‰é’®ä¸€é”®è¡¥å…¨ã€‚")
                if st.button("ğŸ”„ ä¸€é”®è¡¥å…¨ç¼ºå¤±é¡¹", type="primary", key="btn_fill_missing_interviews"):
                    # æ›´æ–° interview_list å’Œ bundle
                    updated_interview_list = []
                    for _, row in df_final.iterrows():
                        dim_name = row["èƒ½åŠ›ç»´åº¦"]
                        question = row["é¢è¯•é¢˜ç›®"]
                        points_str = row["è¯„åˆ†è¦ç‚¹"]
                        
                        # ğŸ”§ å¦‚æœè¿˜æ˜¯"ï¼ˆå¾…ç”Ÿæˆï¼‰"æˆ–ç©ºï¼Œç”ŸæˆçœŸå®é»˜è®¤å†…å®¹ï¼ˆè€Œéæç¤ºè¯­ï¼‰
                        if "ï¼ˆå¾…ç”Ÿæˆï¼‰" in question or not question.strip():
                            question = generate_default_question(dim_name)
                        if not points_str.strip():
                            default_points_list = generate_default_rubric(dim_name)
                            points = default_points_list if isinstance(default_points_list, list) else [str(default_points_list)]
                        else:
                            points = [p.strip() for p in points_str.split("ï¼›") if p.strip()]
                        
                        updated_interview_list.append({
                            "dimension": dim_name,
                            "question": question,
                            "points": points,
                            "score": row.get("åˆ†å€¼", 0)
                        })
                    
                    # æ›´æ–° bundle å’Œ session_state
                    bundle["interview"] = updated_interview_list
                    st.session_state["ai_bundle"] = bundle
                    st.success("âœ… ç¼ºå¤±é¡¹å·²è¡¥å…¨ï¼è¯·åˆ·æ–°é¡µé¢æŸ¥çœ‹æ›´æ–°åçš„è¡¨æ ¼ã€‚")
                    st.rerun()
            else:
                st.markdown("âœ… å„èƒ½åŠ›ç»´åº¦ä¸é¢è¯•é¢˜ç›®å·²å¯¹é½å±•ç¤ºï¼Œä¾¿äºç»“æ„åŒ–è¯„ä¼°ã€‚")
        
            if st.button("ğŸ’¾ å†™å…¥ç³»ç»Ÿï¼ˆä¿å­˜ JD + é¢˜åº“ï¼‰", type="primary", key="btn_save_rubric_1"):
                save_to_system_action()
        else:
            if bundle is None:
                st.info('å°šæœªç”Ÿæˆ Rubricï¼ˆè¯·å…ˆç‚¹å‡»ä¸Šæ–¹"ç”Ÿæˆ JD"ï¼‰')

        # âœ… éšè—è¯„åˆ†ç»´åº¦è§„åˆ™ï¼ˆRubricï¼‰éƒ¨åˆ†ï¼Œåªä¿ç•™åŠŸèƒ½é€»è¾‘
        # è¿™é‡Œä¿ç•™ bundle_for_rubric çš„ç”Ÿæˆå’Œä¿å­˜é€»è¾‘ï¼Œä½†ä¸æ¸²æŸ“åˆ°é¡µé¢
        bundle_for_rubric = st.session_state.get("ai_bundle")
        # bundle_for_rubric å˜é‡ä¿ç•™ï¼Œä¾›å†…éƒ¨é€»è¾‘ä½¿ç”¨ï¼ˆå¦‚ save_to_system_action å‡½æ•°ä¸­ä¼šç”¨åˆ°ï¼‰
        
        # ä¸å†æ˜¾ç¤ºæ ‡é¢˜å’Œå±•å¼€å—ï¼Œé¿å… UI é‡å¤
        # st.subheader("è¯„åˆ†ç»´åº¦è§„åˆ™ï¼ˆRubricï¼‰")  # âŒ æ³¨é‡Šæ‰
        # with st.expander("è¯„åˆ†ç»´åº¦è§„åˆ™ï¼ˆRubricï¼‰", expanded=False):
        #     st.json(bundle_for_rubric["rubric"])
        
        # âœ… ä»…ä¿ç•™ä¸€æ¬¡ä¿å­˜æŒ‰é’®ï¼ˆä¸Šæ–¹å·²æœ‰çš„æŒ‰é’® btn_save_rubric_1ï¼‰
        # å› æ­¤è¿™é‡Œåˆ é™¤é‡å¤æŒ‰é’®ï¼Œé˜²æ­¢é‡å¤æ˜¾ç¤º
        # if st.button("ğŸ’¾ å†™å…¥ç³»ç»Ÿï¼ˆä¿å­˜ JD + é¢˜åº“ï¼‰", type="primary", key="btn_save_rubric_2"):
        #     save_to_system_action()
    
    # ==== AI è¿æ¥è¯Šæ–­ï¼ˆæ”¾åœ¨é¡µé¢åº•éƒ¨ï¼‰====
    with st.expander("ğŸ”§ AI è¿æ¥è¯Šæ–­ï¼ˆæ‰“ä¸å¼€å°±ç‚¹æˆ‘ï¼‰"):
        from backend.services.ai_client import get_client_and_cfg, AIConfig, chat_completion
        
        cfg = AIConfig()
        key_present = bool(cfg.api_key)
        st.write("**å·²æ£€æµ‹åˆ° Keyï¼š**", "âœ…" if key_present else "âŒ")
        if key_present:
            st.write("**Key å‰ç¼€ï¼š**", cfg.api_key[:10] + "..." if len(cfg.api_key) > 10 else cfg.api_key)
        st.write("**Base URLï¼š**", cfg.base_url)
        st.write("**å½“å‰æ¨¡å‹ï¼š**", cfg.model)
        st.write("**Temperatureï¼š**", cfg.temperature)
        
        if st.button("ğŸ§ª æµ‹è¯•ä¸€æ¬¡ AI è¿é€šæ€§"):
            try:
                client, cfg = get_client_and_cfg()
                with st.spinner("æ­£åœ¨æµ‹è¯•è¿æ¥..."):
                    res = chat_completion(
                        client,
                        cfg,
                        messages=[{"role":"user","content":"åªè¿”å› OK"}],
                        temperature=0,
                        max_tokens=10
                    )
                    result = res.choices[0].message.content.strip()
                    st.success(f"âœ… AI è¿é€šæ€§æµ‹è¯•æˆåŠŸï¼è¿”å›ï¼š{result}")
            except Exception as e:
                error_detail = str(e)
                st.error(f"âŒ è¿é€šæ€§å¤±è´¥ï¼š{error_detail}")
                if "Key" in error_detail or "æœªé…ç½®" in error_detail:
                    st.info("ğŸ’¡ æ£€æŸ¥ .env çš„ Key é…ç½®ï¼›ç¡®ä¿æ–‡ä»¶åœ¨é¡¹ç›®æ ¹ç›®å½•ï¼›é‡å¯ Streamlit")
                elif "401" in error_detail or "403" in error_detail:
                    st.info("ğŸ’¡ API Key æ— æ•ˆæˆ–å·²è¿‡æœŸï¼Œè¯·æ£€æŸ¥ .env ä¸­çš„ Key æ˜¯å¦æ­£ç¡®")
                elif "404" in error_detail:
                    st.info("ğŸ’¡ æ¨¡å‹ä¸å­˜åœ¨æˆ–æœªå¼€é€šï¼Œè¯·æ£€æŸ¥ .env ä¸­çš„ AI_MODELï¼Œå°è¯•æ›´æ¢ä¸º Qwen2.5-32B-Instruct")
                elif "timeout" in error_detail.lower() or "è¿æ¥" in error_detail:
                    st.info("ğŸ’¡ ç½‘ç»œè¿æ¥é—®é¢˜ï¼Œæ£€æŸ¥å…¬å¸ç½‘ç»œæ˜¯å¦æ”¾è¡Œ api.siliconflow.cnï¼›æˆ–å°è¯•ä½¿ç”¨ OpenAI")
                else:
                    st.info("ğŸ’¡ æ£€æŸ¥ .env çš„ Key/æ¨¡å‹/Base URLï¼›æˆ–å…¬å¸ç½‘ç»œæ˜¯å¦æ”¾è¡Œ api.siliconflow.cn")
    
    st.markdown("---")
    if SHOW_OFFLINE_SECTION:
        st.markdown("---")
        st.markdown("### ğŸ“‹ ç¦»çº¿è§„åˆ™ç‰ˆï¼ˆå¤‡ç”¨ï¼‰")
        
        # é‡æ–°è¯»å–é…ç½®ï¼ˆå› ä¸ºå¯èƒ½åœ¨ä¾§è¾¹æ å·²æ›´æ–°ï¼‰
        cfg = json.loads(cfg_file.read_text(encoding="utf-8"))
        use_ai = cfg.get("llm_provider") in ["openai", "claude", "siliconflow"]
        
        # è¾“å…¥è¡¨å•ï¼ˆç¦»çº¿ç‰ˆï¼‰
        with st.form("jd_generation_form"):
            col1, col2 = st.columns(2)
            with col1:
                job_name = st.text_input("å²—ä½åç§° *", placeholder="ä¾‹å¦‚ï¼šæ•°æ®åˆ†æå¸ˆã€äº§å“ç»ç†ã€è¿è¥ä¸“å‘˜ç­‰", 
                                        value=st.session_state.get("job_name", ""))
            with col2:
                st.caption("ğŸ’¡ å¿…å¡«é¡¹")
            
            must_have = st.text_area("å¿…å¤‡ç»éªŒ/æŠ€èƒ½", placeholder="ä¾‹å¦‚ï¼š3å¹´ä»¥ä¸Šæ•°æ®åˆ†æç»éªŒï¼›ç†Ÿæ‚‰Pythonã€SQLï¼›æœ‰æ•™è‚²è¡Œä¸šèƒŒæ™¯", 
                                    height=80, help="ç”¨åˆ†å·(;)åˆ†éš”å¤šä¸ªæŠ€èƒ½")
            nice_to_have = st.text_area("åŠ åˆ†é¡¹", placeholder="ä¾‹å¦‚ï¼šç†Ÿæ‚‰æœºå™¨å­¦ä¹ ï¼›æœ‰å›¢é˜Ÿç®¡ç†ç»éªŒï¼›æ•°æ®å¯è§†åŒ–èƒ½åŠ›å¼º", 
                                       height=80, help="ç”¨åˆ†å·(;)åˆ†éš”å¤šä¸ªåŠ åˆ†é¡¹")
            exclude_keywords = st.text_area("æ’é™¤é¡¹", placeholder="ä¾‹å¦‚ï¼šé¢‘ç¹è·³æ§½ï¼›ä»…å®ä¹ ç»éªŒï¼›å¤–åŒ…ç»å†", 
                                           height=60, help="ç”¨åˆ†å·(;)åˆ†éš”å¤šä¸ªæ’é™¤å…³é”®è¯")
            
            submitted = st.form_submit_button("ğŸš€ ç”Ÿæˆ JD", type="primary", use_container_width=True)
        
        # å¤„ç†ç”Ÿæˆè¯·æ±‚
        if submitted:
            if not job_name:
                st.error("âŒ è¯·å¡«å†™å²—ä½åç§°")
            else:
                st.session_state["job_name"] = job_name
                with st.spinner("ğŸ¤– AIæ­£åœ¨æ™ºèƒ½åˆ†æå²—ä½éœ€æ±‚ï¼Œç”Ÿæˆä¸“ä¸šJDã€èƒ½åŠ›ç»´åº¦ã€é¢è¯•é¢˜ç›®ï¼Œè¯·ç¨å€™ï¼ˆé€šå¸¸éœ€è¦10-30ç§’ï¼‰..."):
                    try:
                        jd_long, jd_short, rubric, interview_questions = pipe.generate_jd(
                            job_name, must_have=must_have, nice_to_have=nice_to_have, 
                            exclude_keywords=exclude_keywords, use_ai=use_ai
                        )
                        st.session_state["jd_result"] = (jd_long, jd_short, rubric, interview_questions)
                        st.success("âœ… AIç”ŸæˆæˆåŠŸï¼")
                    except Exception as e:
                        st.error(f"âŒ ç”Ÿæˆå¤±è´¥: {str(e)}")
                        if use_ai:
                            st.info("æ­£åœ¨å°è¯•ä½¿ç”¨ç¦»çº¿æ¨¡å¼...")
                            try:
                                jd_long, jd_short, rubric, interview_questions = pipe.generate_jd(
                                    job_name, must_have=must_have, nice_to_have=nice_to_have, 
                                    exclude_keywords=exclude_keywords, use_ai=False
                                )
                                st.session_state["jd_result"] = (jd_long, jd_short, rubric, interview_questions)
                                st.success("âœ… ç¦»çº¿æ¨¡å¼ç”ŸæˆæˆåŠŸ")
                            except Exception as e2:
                                st.error(f"âŒ ç¦»çº¿æ¨¡å¼ä¹Ÿå¤±è´¥: {str(e2)}")
        
        # æ˜¾ç¤ºç»“æœ
        if "jd_result" in st.session_state:
            jd_long, jd_short, rubric, interview_questions = st.session_state["jd_result"]
            
            # é•¿ç‰ˆJD
            st.markdown("### ğŸ“„ é•¿ç‰ˆ JDï¼ˆBossç›´è˜å¯ç”¨ï¼‰")
            st.text_area("", jd_long, height=300, key="jd_long_display", label_visibility="collapsed")
            
            # çŸ­ç‰ˆJD
            st.markdown("### âœ¨ çŸ­ç‰ˆ JDï¼ˆç¤¾åª’/å†…æ¨ï¼‰")
            st.text_area("", jd_short, height=100, key="jd_short_display", label_visibility="collapsed")
            
            # èƒ½åŠ›ç»´åº¦
            st.markdown("### ğŸ¯ å²—ä½èƒ½åŠ›ç»´åº¦ï¼ˆAIåˆ†æï¼‰")
            if rubric.get("dimensions"):
                dim_data = []
                for dim in rubric["dimensions"]:
                    weight = float(dim.get("weight", 0))
                    dim_data.append({
                        "èƒ½åŠ›ç»´åº¦": dim.get("name", ""),
                        "æƒé‡": f"{weight * 100:.1f}%",
                        "è¯´æ˜": dim.get("description", "")
                    })
                dim_df = pd.DataFrame(dim_data)
                st.dataframe(dim_df, use_container_width=True)
            else:
                st.info('å°šæœªç”Ÿæˆ Rubricï¼ˆè¯·å…ˆç‚¹å‡»ä¸Šæ–¹"ç”Ÿæˆ JD"ï¼‰')
            
            # é¢è¯•é¢˜ç›®
            st.markdown("### ğŸ’¬ é¢è¯•é¢˜ç›®å’Œè¯„åˆ†æ ‡å‡†ï¼ˆAIç”Ÿæˆï¼‰")
            if interview_questions and interview_questions.get("questions"):
                for idx, q in enumerate(interview_questions["questions"], 1):
                    weight_pct = float(q.get('weight', 0)) * 100
                    with st.expander(f"é¢˜ç›® {idx}: {q.get('dimension', 'é€šç”¨')} - æƒé‡: {weight_pct:.0f}%"):
                        st.markdown(f"**é—®é¢˜ï¼š** {q.get('question', '')}")
                        st.markdown(f"**è¯„åˆ†æ ‡å‡†ï¼š** {q.get('evaluation_criteria', '')}")
                        if q.get('weight'):
                            st.caption(f"æƒé‡: {float(q.get('weight', 0)) * 100:.0f}%")
            else:
                st.info("æš‚æ— é¢è¯•é¢˜ç›®")
            
            # ä¿å­˜æŒ‰é’®
            col1, col2 = st.columns([1, 4])
            with col1:
                if st.button("ğŸ’¾ ä¿å­˜ JD & è¯„åˆ†ç»´åº¦", type="primary", key="btn_save_jd_score"):
                    pipe.save_jd(job_name, jd_long, jd_short, rubric, interview_questions)
                    st.success("âœ… å·²ä¿å­˜")

with tab2:
    st.subheader("å¯¼å…¥ç®€å†ï¼ˆCSV/TXT ç¤ºä¾‹ï¼‰å¹¶åŒ¹é…æ‰“åˆ†")
    uploaded = st.file_uploader("ä¸Šä¼ ç®€å† CSVï¼ˆè§ data/samples/sample_resumes.csvï¼‰æˆ– TXTï¼ˆå•ä¸ªï¼‰", type=["csv","txt"], accept_multiple_files=True)
    if uploaded:
        for f in uploaded:
            if f.name.endswith(".csv"):
                df = pd.read_csv(f); pipe.ingest_resumes_df(df)
            else:
                txt = f.read().decode("utf-8", errors="ignore"); pipe.ingest_text_resume(txt)
        st.success("å·²å¯¼å…¥")
    if st.button("æ‰¹é‡è¯„åˆ†"):
        start = time.time()
        result_df = pipe.score_all(st.session_state.get("job_name"))
        st.session_state["scored"] = result_df
        st.info(f"è¯„åˆ†å®Œæˆï¼Œç”¨æ—¶ {time.time()-start:.2f} s")
        # æ±‰åŒ–æ˜¾ç¤º
        result_df_display = translate_dataframe_columns(result_df)
        st.dataframe(result_df_display, use_container_width=True)

    st.markdown("---")
    st.markdown("## ğŸ¤– AI æ™ºèƒ½åŒ¹é…ï¼ˆæ‰¹é‡ä¸Šä¼  PDF/DOCX/å›¾ç‰‡ï¼‰")

    jd_text = ""
    if st.session_state.get("ai_bundle") and st.session_state["ai_bundle"].get("jd_long"):
        jd_text = st.session_state["ai_bundle"]["jd_long"]

    jd_text = st.text_area(
        "å²—ä½ JD æ–‡æœ¬ï¼ˆå·²è‡ªåŠ¨å¸¦å…¥ AI é•¿ç‰ˆ JDï¼Œå¯æ‰‹åŠ¨ç¼–è¾‘ï¼‰",
        value=jd_text,
        height=200,
        help="AI ä¼šåŸºäºè¿™é‡Œçš„ JD ä¸ç®€å†è¿›è¡ŒåŒ¹é…ï¼Œè¯·ç¡®ä¿å†…å®¹å‡†ç¡®ã€‚"
    )

    uploaded_files = st.file_uploader(
        "ä¸Šä¼ å¤šä»½ç®€å†ï¼ˆæ”¯æŒï¼špdfã€docxã€txtã€jpgã€jpegã€pngï¼‰",
        type=["pdf", "docx", "txt", "jpg", "jpeg", "png"],
        accept_multiple_files=True,
        key="ai_resume_uploader"
    )

    if uploaded_files:
        with st.spinner("æ­£åœ¨è§£æç®€å†æ–‡ä»¶â€¦"):
            resumes_df = parse_uploaded_files_to_df(uploaded_files)
        if resumes_df.empty:
            st.warning("æ²¡æœ‰è§£æåˆ°æœ‰æ•ˆç®€å†ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶æ ¼å¼ã€‚")
        else:
            st.success(f"å·²è§£æ {len(resumes_df)} ä»½ç®€å†ã€‚")
            st.dataframe(
                resumes_df[["candidate_id", "file", "email", "phone", "text_len"]],
                use_container_width=True
            )

            if st.button("ğŸš€ ç”¨ AI æ‰¹é‡åŒ¹é…å¹¶æ‰“åˆ†"):
                if not jd_text.strip():
                    st.warning("è¯·å…ˆå¡«å†™/ç²˜è´´å²—ä½ JDã€‚")
                else:
                    # è·å–å²—ä½åç§°ï¼Œç”¨äºå²—ä½çº§æ¸…æ´—é€»è¾‘
                    job_title = st.session_state.get("job_name", "")
                    with st.spinner("AI æ­£åœ¨æ™ºèƒ½åˆ†æåŒ¹é…åº¦ï¼Œè¯·ç¨å€™â€¦"):
                        scored_df = ai_match_resumes_df(jd_text, resumes_df, job_title)
                    st.dataframe(
                        scored_df[[
                            "candidate_id",
                            "file",
                            "email",
                            "phone",
                            "æ€»åˆ†",
                            "æŠ€èƒ½åŒ¹é…åº¦",
                            "ç»éªŒç›¸å…³æ€§",
                            "æˆé•¿æ½œåŠ›",
                            "ç¨³å®šæ€§",
                            "ç®€è¯„",
                            "è¯æ®"
                        ]],
                        use_container_width=True
                    )
                    result_df = scored_df

                    # âœ… ä¸€é”®ä¿®å¤ç‰ˆï¼šAI åŒ¹é…å®Œæˆåè‡ªåŠ¨ä¿å­˜ & è·³è½¬

                    # åˆ¤æ–­AIåŒ¹é…ç»“æœæ˜¯å¦ä¸ºç©º
                    if "result_df" in locals() and not result_df.empty:
                        # ä¿å­˜è¯„åˆ†ç»“æœåˆ°session_stateï¼Œä¾›ä¸‹ä¸€æ­¥â€œå»é‡&æ’åºâ€ä½¿ç”¨
                        st.session_state["score_df"] = result_df
                        st.session_state["scored"] = result_df

                        # æ˜¾ç¤ºæˆåŠŸæç¤º
                        st.success("AI åŒ¹é…åˆ†æå®Œæˆ âœ…")
                        st.info("ç³»ç»Ÿå·²è‡ªåŠ¨ä¿å­˜è¯„åˆ†ç»“æœï¼Œè¯·ç‚¹å‡»é¡¶éƒ¨å¯¼èˆªæ ã€3 å»é‡ & æ’åºã€æŸ¥çœ‹ Top-N å€™é€‰äººã€‚")

                        # è‡ªåŠ¨å¯¼å‡ºCSVæ–‡ä»¶åˆ°é¡¹ç›®dataç›®å½•
                        import os
                        output_path = os.path.join("data", "ai_match_results.csv")
                        try:
                            result_df.to_csv(output_path, index=False, encoding="utf-8-sig")
                            st.write(f"âœ… å·²è‡ªåŠ¨ä¿å­˜åŒ¹é…ç»“æœè‡³ `{output_path}`")
                        except Exception as e:
                            st.warning(f"âš ï¸ ä¿å­˜CSVå¤±è´¥: {e}")

                        # ï¼ˆå¯é€‰ï¼‰æä¾›ä¸‹è½½æŒ‰é’®
                        st.download_button(
                            label="â¬‡ï¸ ä¸‹è½½ AI åŒ¹é…ç»“æœï¼ˆCSVï¼‰",
                            data=result_df.to_csv(index=False).encode("utf-8-sig"),
                            file_name="ai_match_results.csv",
                            mime="text/csv"
                        )
                    else:
                        st.warning("âš ï¸ æš‚æ— åŒ¹é…ç»“æœï¼Œè¯·å…ˆå®ŒæˆAIåŒ¹é…è¯„åˆ†åå†å°è¯•ã€‚")
    else:
        st.info("è¯·ä¸Šä¼ ä¸€æ‰¹ç®€å†æ–‡ä»¶å¼€å§‹åˆ†æã€‚")

with tab3:
    st.subheader("å»é‡ & æ’åºï¼ˆå±•ç¤º Top-Nï¼‰")
    topn = st.slider("Top-N", 5, 50, 10)
    score_source = None
    if "score_df" in st.session_state:
        score_source = st.session_state["score_df"]
    elif "scored" in st.session_state:
        score_source = st.session_state["scored"]

    if score_source is not None:
        deduped = pipe.dedup_and_rank(score_source)
        st.session_state["shortlist"] = deduped.head(topn)
        # æ±‰åŒ–æ˜¾ç¤º
        deduped_display = translate_dataframe_columns(deduped.head(topn))
        st.dataframe(deduped_display, use_container_width=True)
    else:
        st.warning("è¯·å…ˆå®Œæˆè¯„åˆ†")

with tab4:
    st.subheader("ğŸ¤– ä¸€é”®é‚€çº¦ + è‡ªåŠ¨æ’æœŸ")
    st.markdown("è®©AIå¸®ä½ ç”Ÿæˆä¸ªæ€§åŒ–é‚€çº¦é‚®ä»¶ï¼ˆå«å€™é€‰äº®ç‚¹ + æ—¥å†é™„ä»¶ï¼‰")

    score_df = st.session_state.get("score_df")
    if score_df is None or score_df.empty:
        st.warning("è¯·å…ˆå®ŒæˆAIåŒ¹é…è¯„åˆ†ã€‚")
    else:
        df = score_df.copy()
        max_candidates = len(df)
        default_top = min(5, max_candidates)
        top_n = st.number_input(
            "é€‰æ‹©è¦é‚€çº¦çš„å€™é€‰äººæ•°ï¼ˆTop-Nï¼‰",
            min_value=1,
            max_value=max_candidates,
            value=default_top,
            step=1,
        )
        top_n = int(top_n)
        selected_candidates = df.head(top_n)

        score_col = "æ€»åˆ†" if "æ€»åˆ†" in df.columns else "score_total" if "score_total" in df.columns else None
        display_cols = [col for col in ["file", "email", score_col] if col and col in df.columns]
        if not display_cols:
            display_cols = df.columns.tolist()

        st.write(f"å·²é€‰æ‹© {top_n} ä½å€™é€‰äººï¼š")
        st.dataframe(selected_candidates[display_cols], use_container_width=True)

        interview_time = st.text_input("ğŸ•’ é¢è¯•æ—¶é—´ï¼ˆä¾‹ï¼š2025-11-15 14:00, Asia/Shanghaiï¼‰", "2025-11-15 14:00, Asia/Shanghai")
        organizer_email = st.text_input("ğŸ“§ é¢è¯•ç»„ç»‡è€…é‚®ç®±", "hr@company.com")

        if st.button("ğŸš€ ä¸€é”®ç”Ÿæˆé‚€çº¦é‚®ä»¶ + ICS"):
            st.info("AI æ­£åœ¨ç”Ÿæˆä¸ªæ€§åŒ–é‚€çº¦å†…å®¹ï¼Œè¯·ç¨å€™...")

            invite_results = []
            invites_dir = "reports/invites"
            os.makedirs(invites_dir, exist_ok=True)

            job_title = st.session_state.get("job_name") or "ç›®æ ‡å²—ä½"

            for _, row in selected_candidates.iterrows():
                row_dict = row.to_dict()
                candidate_name = row_dict.get("file") or row_dict.get("name") or "åŒ¿åå€™é€‰äºº"
                candidate_email = row_dict.get("email", "")
                candidate_score = row_dict.get("æ€»åˆ†") or row_dict.get("score_total") or row_dict.get("score", "æœªçŸ¥")

                try:
                    candidate_highlight = generate_ai_summary(row_dict)
                except Exception as e:
                    candidate_highlight = f"AI æ€»ç»“å¤±è´¥ï¼š{e}"

                try:
                    ics_path = create_ics_file(
                        title=f"é¢è¯•é‚€çº¦ - {candidate_name}",
                        start_time=interview_time,
                        organizer=organizer_email,
                        attendee=candidate_email or "candidate@example.com",
                    )
                except Exception as e:
                    st.warning(f"ç”Ÿæˆ {candidate_name} çš„æ—¥å†æ–‡ä»¶å¤±è´¥ï¼š{e}")
                    ics_path = ""

                try:
                    email_body = generate_ai_email(
                        name=candidate_name,
                        highlights=candidate_highlight,
                        position=job_title,
                        score=candidate_score,
                        ics_path=ics_path or "(é™„ä»¶ç”Ÿæˆå¤±è´¥)",
                    )
                except Exception as e:
                    email_body = f"AI é‚®ä»¶ç”Ÿæˆå¤±è´¥ï¼š{e}"

                invite_results.append(
                    {
                        "name": candidate_name,
                        "email": candidate_email,
                        "ics": ics_path,
                        "body": email_body,
                        "highlights": candidate_highlight,
                        "score": candidate_score,
                    }
                )

            json_payload = json.dumps(invite_results, ensure_ascii=False, indent=2)
            json_path = os.path.join(invites_dir, f"invite_batch_{datetime.now().strftime('%Y%m%d_%H%M')}.json")
            with open(json_path, "w", encoding="utf-8") as fp:
                fp.write(json_payload)

            st.success("âœ… AI ä¸ªæ€§åŒ–é‚€çº¦ç”Ÿæˆå®Œæˆï¼")
            st.download_button(
                "ğŸ“¥ ä¸‹è½½é‚€çº¦ç»“æœï¼ˆJSONï¼‰",
                data=json_payload,
                file_name="ai_invites.json",
                mime="application/json",
            )

            pending_path = "reports/pending_interviews.csv"
            pd.DataFrame(invite_results).to_csv(pending_path, index=False, encoding="utf-8-sig")
            st.write(f"ğŸ“‹ å·²è‡ªåŠ¨æ›´æ–°å¾…é¢è¯•æ¸…å•ï¼š`{pending_path}`")

            st.json(invite_results, expanded=False)

with tab5:
    st.subheader("é¢è¯•åŒ… & å¯¼å‡ºæŠ¥è¡¨")
    if st.button("å¯¼å‡ºæœ¬è½®æŠ¥è¡¨"):
        score_df = st.session_state.get("score_df", None)
        scored_df = st.session_state.get("scored", None)

        if score_df is not None and not score_df.empty:
            score_source = score_df
        elif scored_df is not None and not scored_df.empty:
            score_source = scored_df
        else:
            st.warning("æœªæ‰¾åˆ°å¯å¯¼å‡ºçš„è¯„åˆ†æ•°æ®ï¼Œè¯·å…ˆå®Œæˆ AI åŒ¹é…è¯„åˆ†ã€‚")
            st.stop()

        path = export_round_report(score_source)
        st.success("å·²å¯¼å‡ºï¼š" + path)

