import json
import os
import re
import time

import pandas as pd
import streamlit as st
from datetime import datetime
from pathlib import Path
from backend.storage.db import init_db, get_db
from backend.services.pipeline import RecruitPipeline
from backend.services.reporting import export_round_report
from backend.utils.versioning import VersionManager
from backend.utils.field_mapping import translate_dataframe_columns, translate_field
# å¼ºåˆ¶é‡æ–°åŠ è½½æ¨¡å—ï¼Œé¿å…ç¼“å­˜é—®é¢˜
import importlib
import sys
if 'backend.services.jd_ai' in sys.modules:
    importlib.reload(sys.modules['backend.services.jd_ai'])
from backend.services.jd_ai import generate_jd_bundle, construct_full_ability_list
from backend.services.resume_parser import parse_uploaded_files_to_df
# ğŸ”„ ç¡®ä¿ AI åŒ¹é…é€»è¾‘æ›´æ–°æ—¶ç«‹å³ç”Ÿæ•ˆ
if 'backend.services.ai_matcher' in sys.modules:
    importlib.reload(sys.modules['backend.services.ai_matcher'])
from backend.services.ai_matcher import ai_match_resumes_df
from backend.services.ai_core import generate_ai_summary, generate_ai_email
from backend.services.calendar_utils import create_ics_file
# from backend.services.excel_exporter import generate_competency_excel, export_ability_sheet_to_file  # å‡½æ•°ä¸å­˜åœ¨ï¼Œå·²æ³¨é‡Š

# å¼ºåˆ¶é‡æ–°åŠ è½½ Excel å¯¼å‡ºæ¨¡å—ï¼Œç¡®ä¿æ¨¡æ¿æ ·å¼è°ƒæ•´åå‰ç«¯ç«‹å³ç”Ÿæ•ˆ
if 'backend.services.export_excel' in sys.modules:
    importlib.reload(sys.modules['backend.services.export_excel'])
from backend.services.export_excel import export_competency_excel
from dotenv import load_dotenv

# å°è¯•ä»å¤šä¸ªä½ç½®åŠ è½½.envæ–‡ä»¶
env_paths = [
    Path('.env'),  # å½“å‰ç›®å½•ï¼ˆapp/ï¼‰
    Path('../.env'),  # é¡¹ç›®æ ¹ç›®å½•
    Path(__file__).parent.parent / '.env',  # é¡¹ç›®æ ¹ç›®å½•ï¼ˆç»å¯¹è·¯å¾„ï¼‰
]
for env_path in env_paths:
    if env_path.exists():
        load_dotenv(dotenv_path=env_path)
        break
else:
    load_dotenv()  # é»˜è®¤åŠ è½½

# --- session åˆå§‹åŒ–ï¼ˆæ”¾åœ¨ import ä¹‹åï¼‰---
if "ai_bundle" not in st.session_state:
    st.session_state["ai_bundle"] = None

# ============ æ§åˆ¶æ˜¾ç¤ºéƒ¨åˆ† ============
SHOW_OFFLINE_SECTION = False   # æ˜¯å¦æ˜¾ç¤ºâ€œç¦»çº¿è§„åˆ™ç‰ˆâ€
SHOW_DETAIL_SECTIONS = True   # æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†éƒ¨åˆ†ï¼ˆé•¿ç‰ˆJD / å²—ä½èƒ½åŠ›ç»´åº¦ / é¢è¯•é¢˜ç­‰ï¼‰
# =====================================

st.set_page_config(page_title="RecruitFlow | ä¸€é”®æ‹›è˜æµæ°´çº¿", layout="wide")
st.title("RecruitFlow â€” ä¸€é”®æ‹›è˜æµæ°´çº¿ï¼ˆæ•™è‚²æœºæ„ç‰ˆï¼‰")

def sanitize_single_line(text, default="æœªæä¾›ç›¸å…³ä¿¡æ¯", limit=None):
    if text is None:
        return default
    cleaned = str(text).replace("\r", " ").replace("\n", "ï¼›")
    cleaned = re.sub(r"\s+", " ", cleaned)
    cleaned = (
        cleaned.replace(",", "ï¼Œ")
        .replace(";", "ï¼›")
        .replace("|", "ï½œ")
        .strip(" ï¼›")
    )
    if not cleaned:
        cleaned = default
    if limit and len(cleaned) > limit:
        cleaned = cleaned[:limit].rstrip("ï¼› ï¼Œ") + "..."
    return cleaned


def _clean_single_line(text, default="æœªæä¾›", limit=None):
    return sanitize_single_line(text, default, limit)


def _format_highlights_for_export(row_dict):
    tags = []
    raw = row_dict.get("highlights")
    if isinstance(raw, str):
        tags = [seg.strip() for seg in re.split(r"[ï½œ|ï¼Œ,ã€ï¼›\s]+", raw) if seg.strip()]
    elif isinstance(raw, list):
        tags = [str(seg).strip() for seg in raw if str(seg).strip()]

    if len(tags) < 2:
        strengths_text = row_dict.get("short_eval", "")
        if strengths_text:
            candidates = [seg.strip(" ï¼›;") for seg in re.split(r"[ï½œ|ï¼Œ,ã€ï¼›\s]+", strengths_text) if seg.strip()]
            for item in candidates:
                if len(item) <= 8:
                    tags.append(item)
                if len(tags) >= 3:
                    break

    tags = [tag for tag in tags if tag][:3]
    if len(tags) == 1:
        tags.append("ç»¼åˆèƒ½åŠ›")
    return "ï½œ".join(tags) if tags else "æœªæä¾›"


def _safe_load_json(value):
    if isinstance(value, dict):
        return value
    if isinstance(value, str):
        try:
            return json.loads(value)
        except Exception:
            return {}
    return {}


def _format_resume_summary(row_dict):
    summary = row_dict.get("resume_mini") or row_dict.get("short_eval") or "æœªæä¾›ç›¸å…³ä¿¡æ¯"
    return _clean_single_line(summary, default="æœªæä¾›ç›¸å…³ä¿¡æ¯", limit=130)


def _format_evidence_field(row_dict):
    reasoning = _safe_load_json(row_dict.get("reasoning_chain"))
    short_eval_struct = _safe_load_json(row_dict.get("short_eval_struct"))

    def _format_strengths():
        chain = reasoning.get("strengths_reasoning_chain") or []
        entries = []
        for idx, item in enumerate(chain, 1):
            if not isinstance(item, dict):
                continue
            conclusion = _clean_single_line(item.get("conclusion"), "æœªå‘½åä¼˜åŠ¿", 18)
            actions = _clean_single_line(item.get("detected_actions"), "æœªæä¾›", 24)
            evidence = _clean_single_line(item.get("resume_evidence"), "æœªæä¾›", 48)
            reasoning_txt = _clean_single_line(item.get("ai_reasoning"), "æœªæä¾›", 36)
            entries.append(f"{idx}. {conclusion}ï½œåŠ¨ä½œ:{actions}ï½œè¯æ®:{evidence}ï½œæ¨æ–­:{reasoning_txt}")
        return "ï¼›".join(entries) if entries else "æš‚æ— å¯éªŒè¯ä¼˜åŠ¿"

    def _format_weaknesses():
        chain = reasoning.get("weaknesses_reasoning_chain") or []
        entries = []
        for idx, item in enumerate(chain, 1):
            if not isinstance(item, dict):
                continue
            conclusion = _clean_single_line(item.get("conclusion"), "æœªå‘½ååŠ£åŠ¿", 18)
            gap = _clean_single_line(item.get("resume_gap"), "æœªæä¾›", 32)
            compare = _clean_single_line(item.get("compare_to_jd"), "æœªæä¾›", 40)
            risk = _clean_single_line(item.get("ai_reasoning"), "æœªæä¾›", 36)
            entries.append(f"{idx}. {conclusion}ï½œç¼ºå£:{gap}ï½œJD:{compare}ï½œé£é™©:{risk}")
        return "ï¼›".join(entries) if entries else "æš‚æ— å¯éªŒè¯åŠ£åŠ¿"

    match_level = _clean_single_line(short_eval_struct.get("match_level"), "æ— æ³•è¯„ä¼°", 4)
    match_reason = _clean_single_line(short_eval_struct.get("match_reason"), "æœªæä¾›åŒ¹é…åŸå› ", 60)
    match_text = f"{match_level}ï¼š{match_reason}"

    evidence_text = f"ã€ä¼˜åŠ¿ã€‘{_format_strengths()}ã€åŠ£åŠ¿ã€‘{_format_weaknesses()}ã€åŒ¹é…åº¦ã€‘{match_text}"
    return _clean_single_line(evidence_text, default="æœªæä¾›")


def _build_export_dataframe(result_df, job_title):
    rows = []
    position_name = _clean_single_line(job_title, default="æœªæä¾›")
    for _, row in result_df.iterrows():
        row_dict = row.to_dict()
        candidate_id = row_dict.get("candidate_id")

        try:
            candidate_id = int(candidate_id)
        except Exception:
            candidate_id = 0

        export_row = {
            "å€™é€‰äººID": candidate_id,
            "å§“å": _clean_single_line(row_dict.get("name"), "æœªæä¾›"),
            "æ–‡ä»¶å": _clean_single_line(row_dict.get("file"), "æœªæä¾›"),
            "å²—ä½": position_name,
            "é‚®ç®±": _clean_single_line(row_dict.get("email"), "æœªæä¾›"),
            "æ‰‹æœºå·": _clean_single_line(row_dict.get("phone"), "æœªæä¾›"),
            "æ€»åˆ†": int(round(float(row_dict.get("æ€»åˆ†", 0)))),
            "äº®ç‚¹": _format_highlights_for_export(row_dict),
            "ç®€å†æ‘˜è¦": _format_resume_summary(row_dict),
            "è¯æ®": _format_evidence_field(row_dict),
        }
        rows.append(export_row)
    return pd.DataFrame(rows)


with st.sidebar:
    st.header("è®¾ç½®")
    cfg_file = Path("backend/configs/model_config.json")
    cfg = json.loads(cfg_file.read_text(encoding="utf-8"))
    
    # AIé…ç½®ï¼ˆé”å®šä¸ºGPT-4ï¼‰
    st.subheader("AIé…ç½®")
    st.markdown("**AIæä¾›å•†ï¼š** OpenAI (å·²é”å®š)")
    st.markdown("**æ¨¡å‹åç§°ï¼š** GPT-4 (å·²é”å®š)")
    st.info("ğŸ”’ AIé…ç½®å·²é”å®šä¸ºGPT-4ï¼Œç¡®ä¿ç”Ÿæˆè´¨é‡ã€‚å¦‚éœ€ä¿®æ”¹ï¼Œè¯·è”ç³»ç®¡ç†å‘˜ã€‚")
    st.caption("ğŸ’¡ è¯·è®¾ç½®ç¯å¢ƒå˜é‡: OPENAI_API_KEY æˆ–é…ç½® backend/configs/api_keys.json")
    
    # å›ºå®šä½¿ç”¨GPT-4
    llm_provider = "openai"
    llm_model = "gpt-4"
    
    st.markdown("---")
    
    # å…¶ä»–è®¾ç½®
    st.subheader("ç­›é€‰è®¾ç½®")
    
    blind = st.toggle("ç›²ç­›æ¨¡å¼ï¼ˆéšè—å§“å/å­¦æ ¡ç­‰ï¼‰", value=cfg.get("blind_screen", True),
                     help="å¼€å¯åï¼Œåœ¨ç®€å†ç­›é€‰è¿‡ç¨‹ä¸­éšè—å€™é€‰äººçš„å§“åã€å­¦æ ¡ç­‰æ•æ„Ÿä¿¡æ¯ï¼Œé¿å…å› ä¸ªäººèƒŒæ™¯äº§ç”Ÿåè§ï¼Œç¡®ä¿å…¬å¹³ç­›é€‰")
    
    thr = st.slider("ç½®ä¿¡åº¦é˜ˆå€¼", 0.0, 1.0, cfg.get("confidence_threshold", 0.65), 0.05,
                    help="è¯„åˆ†ç½®ä¿¡åº¦ä½äºæ­¤é˜ˆå€¼çš„å€™é€‰äººå°†è¢«æ ‡è®°ä¸º'é˜ˆå€¼æ‹¦æˆª'ï¼Œä¸ä¼šè‡ªåŠ¨å‘é€é‚€çº¦ã€‚å»ºè®®å€¼ï¼š0.6-0.7ã€‚å€¼è¶Šé«˜ï¼Œç­›é€‰è¶Šä¸¥æ ¼ã€‚")
    
    st.caption("ğŸ’¡ ç½®ä¿¡åº¦é˜ˆå€¼è¯´æ˜ï¼šç³»ç»Ÿä¼šæ ¹æ®ç®€å†åŒ¹é…åº¦è®¡ç®—ä¸€ä¸ªç½®ä¿¡åº¦åˆ†æ•°ã€‚ä½äºé˜ˆå€¼çš„å€™é€‰äººéœ€è¦äººå·¥å®¡æ ¸åæ‰èƒ½é‚€çº¦ã€‚")
    if st.button("ä¿å­˜è®¾ç½®"):
        cfg["blind_screen"] = blind
        cfg["confidence_threshold"] = float(thr)
        # AIé…ç½®å·²é”å®šï¼Œä¸æ›´æ–°
        cfg["llm_provider"] = "openai"
        cfg["llm_model"] = "gpt-4"
        cfg_file.write_text(json.dumps(cfg, ensure_ascii=False, indent=2), encoding="utf-8")
        st.success("âœ… è®¾ç½®å·²ä¿å­˜ï¼ˆAIé…ç½®ä¿æŒé”å®šä¸ºGPT-4ï¼‰")
    st.markdown("---"); st.caption("ç‰ˆæœ¬æ§åˆ¶")
    vm = VersionManager()
    if st.button("åˆ›å»ºå¿«ç…§"):
        st.success("å·²åˆ›å»ºç‰ˆæœ¬ï¼š" + vm.snapshot())


init_db(); pipe = RecruitPipeline()
tab1, tab2, tab3, tab4, tab5 = st.tabs(["1 ç”Ÿæˆ JD","2 ç®€å†è§£æ & åŒ¹é…","3 å»é‡ & æ’åº","4 é‚€çº¦ & æ’æœŸ","5 é¢è¯•åŒ… & å¯¼å‡º"])

with tab1:
    # ==========================================================
    # âœ… ç»Ÿä¸€æŒ‰é’®å®šä¹‰ï¼Œé˜²æ­¢ StreamlitDuplicateElementId é”™è¯¯
    # ==========================================================
    
    # ğŸ”¹ åŠŸèƒ½ï¼šä¿å­˜ JD ä¸ Rubric æ•°æ®
    # ğŸ”¹ ä¿®å¤ï¼šä¸ºæŒ‰é’®åˆ†é…å”¯ä¸€ keyï¼Œé˜²æ­¢é‡å¤ ID å†²çª
    # ğŸ”¹ æµ‹è¯•ç»“æœï¼šå·²é€šè¿‡å¤šæ¬¡ Cursor ä¸ Streamlit è¿è¡ŒéªŒè¯ï¼ˆæ— å¼‚å¸¸ï¼‰
    # ==========================================================
    
    # å°è£…æŒ‰é’®è¡Œä¸ºï¼ˆå¯å¤ç”¨ï¼‰
    def save_to_system_action():
        """ç»Ÿä¸€çš„ä¿å­˜ JD + é¢˜åº“æ“ä½œ"""
        current_bundle = st.session_state.get("ai_bundle")
        if not current_bundle:
            st.warning('è¯·å…ˆç‚¹å‡»"ç”Ÿæˆ JD"è·å¾— AI ç»“æœã€‚')
            return
        
        try:
            job_to_save = (st.session_state.get("job_name") or "").strip()
            if not job_to_save:
                job_to_save = current_bundle.get("rubric", {}).get("job", "")
    
            pipe.save_jd(job_to_save, current_bundle["jd_long"], current_bundle["jd_short"], current_bundle["rubric"])
    
            q_path = Path("data/templates/é¢˜åº“ç¤ºä¾‹.csv")
            rows = []
            for q in current_bundle.get("interview", []):
                points = q.get("points") or []
                points_str = "ï¼›".join(points) if isinstance(points, list) else (str(q.get("points", "")) if q.get("points") else "")
                rows.append({
                    "job": job_to_save,
                    "èƒ½åŠ›ç»´åº¦": q.get("dimension", "é€šç”¨"),
                    "é¢˜ç›®": q.get("question", ""),
                    "è¯„åˆ†è¦ç‚¹": points_str,
                    "åˆ†å€¼": int(q.get("score", 0)),
                    "æƒé‡": round(float(q.get("score", 0)) / 100.0, 4)
                })
            if rows:
                qdf = pd.DataFrame(rows)
                q_path.parent.mkdir(parents=True, exist_ok=True)
                header = not q_path.exists()
                qdf.to_csv(q_path, mode="a", index=False, encoding="utf-8-sig", header=header)
            st.success("å·²å†™å…¥ï¼šJD / Rubric / é¢˜åº“")
        except Exception as e:
            st.error(f"âŒ å†™å…¥å¤±è´¥ï¼š{e}")
    
    st.subheader("æ™ºèƒ½ç”Ÿæˆ JDï¼ˆAIåˆ†æï¼‰")
    
    # === æ–°å¢ï¼šæ™ºèƒ½ç”Ÿæˆ JDï¼ˆAI åˆ†æï¼‰ ===
    st.markdown("### ğŸ¤– æ™ºèƒ½ç”Ÿæˆ JDï¼ˆAI åˆ†æï¼‰")
    
    # é¢„æ£€æŸ¥ï¼šAI Key
    key_present = bool(os.getenv("SILICONFLOW_API_KEY") or os.getenv("OPENAI_API_KEY"))
    if not key_present:
        st.warning("âš ï¸ æœªæ£€æµ‹åˆ° AI Keyï¼šè¯·åœ¨é¡¹ç›®æ ¹ç›®å½•åˆ›å»º `.env` å¹¶é…ç½® SILICONFLOW_API_KEYã€‚")
    
    with st.form("ai_jd_form"):
        ai_job = st.text_input("å²—ä½åç§° *", value=st.session_state.get("job_name",""), help="ä¾‹å¦‚ï¼šæ•°å­¦ç«èµ›æ•™ç»ƒ/æ•™å­¦è¿è¥ä¸“å‘˜/ç­ä¸»ä»»/Javaåç«¯")
        ai_must = st.text_area("å¿…å¤‡ç»éªŒ/æŠ€èƒ½", value="", height=80, help="åˆ†å·æˆ–ç©ºæ ¼åˆ†éš”ï¼Œä¾‹å¦‚ï¼šå›½ä¸€; LaTeX; IMOè®­ç»ƒ")
        ai_nice = st.text_area("åŠ åˆ†é¡¹", value="", height=60, help="å¦‚ï¼šç«èµ›å‡ºé¢˜ç»éªŒ; å…¬å¼€è¯¾; å†…å®¹åˆ¶ä½œ")
        ai_excl = st.text_area("æ’é™¤é¡¹", value="", height=60, help="å¦‚ï¼šä»…å®ä¹ ; å…¼èŒ")
        submitted = st.form_submit_button("ğŸš€ ç”Ÿæˆ JD", type="primary", use_container_width=True)
        
        if submitted:
            if not ai_job:
                st.error("âŒ è¯·å¡«å†™å²—ä½åç§°")
            else:
                st.session_state["job_name"] = ai_job
                # è¾“å…¥æ¸…æ´—ï¼štex -> LaTeX
                ai_must = ai_must.replace("tex", "LaTeX").replace("Tex", "LaTeX")
                ai_nice = ai_nice.replace("tex", "LaTeX").replace("Tex", "LaTeX")
                try:
                    # å¼ºåˆ¶é‡æ–°åŠ è½½æ¨¡å—ï¼Œç¡®ä¿ä½¿ç”¨æœ€æ–°ä»£ç 
                    if 'backend.services.jd_ai' in sys.modules:
                        importlib.reload(sys.modules['backend.services.jd_ai'])
                        from backend.services.jd_ai import generate_jd_bundle
                    with st.spinner("ğŸ¤– AIæ­£åœ¨æ™ºèƒ½åˆ†æå²—ä½éœ€æ±‚ï¼Œç”Ÿæˆä¸“ä¸šJDã€èƒ½åŠ›ç»´åº¦ã€é¢è¯•é¢˜ç›®ï¼Œè¯·ç¨å€™ï¼ˆé€šå¸¸éœ€è¦10-30ç§’ï¼‰..."):
                        bundle = generate_jd_bundle(ai_job, ai_must, ai_nice, ai_excl)
                        # åŸºäºé•¿ç‰ˆ JD å†åšä¸€æ¬¡â€œçŸ­ç‰ˆJDæå– + ä»»èŒè¦æ±‚æŠ½å–èƒ½åŠ›ä¸é¢è¯•é¢˜â€
                        from backend.services.jd_ai import extract_short_and_competencies_from_long_jd
                        extracted = extract_short_and_competencies_from_long_jd(bundle.get("jd_long",""), ai_job)
                        if extracted:
                            # âœ… ä¸å†ç”¨æŠ½å–å¾—åˆ°çš„çŸ­ç‰ˆ JD è¦†ç›–ï¼Œä»¥å…ç ´åâ€œå°çº¢ä¹¦é£æ ¼â€çŸ­ç‰ˆ JD
                            # å¦‚éœ€æŸ¥çœ‹æŠ½å–ç‰ˆçŸ­ JDï¼Œå¯åç»­å•ç‹¬åœ¨å‰ç«¯å±•ç¤º extracted["short_jd"]
                            # ç”¨æŠ½å–å¾—åˆ°çš„èƒ½åŠ›ç»´åº¦/é¢è¯•é¢˜è¦†ç›–å±•ç¤ºï¼ˆè½¬æ¢ä¸ºå†…éƒ¨æ ¼å¼ï¼‰
                            dims = []
                            for d in extracted.get("èƒ½åŠ›ç»´åº¦", []):
                                anchors = d.get("è¯„åˆ†é”šç‚¹") or {}
                                dims.append({
                                    "name": d.get("ç»´åº¦åç§°", ""),
                                    "weight": round(float(d.get("æƒé‡", 0)) / 100.0, 4),
                                    "desc": d.get("å®šä¹‰", ""),
                                    "anchors": {
                                        "20": anchors.get("20") or "åŸºç¡€è¾¾æˆï¼šè¯·ç»“åˆ JD ä¸­çš„åŸºç¡€è¦æ±‚æè¿°ã€‚",
                                        "60": anchors.get("60") or "è‰¯å¥½è¾¾æˆï¼šèƒ½å¤Ÿç¨³å®šäº§å‡ºå¹¶ä¸æ–­ä¼˜åŒ–ã€‚",
                                        "100": anchors.get("100") or "ä¼˜ç§€è¾¾æˆï¼šæŒç»­è¾“å‡ºæ°å‡ºæˆæœå¹¶é‡åŒ–å½±å“ã€‚",
                                    },
                                })
                            if dims:
                                bundle["dimensions"] = dims
                            qs = []
                            for q in extracted.get("èƒ½åŠ›ç»´åº¦_é¢è¯•é¢˜", []):
                                raw_points = q.get("è¯„åˆ†è¦ç‚¹", [])
                                if isinstance(raw_points, str):
                                    points_list = [p.strip() for p in re.split(r"[ï¼›;ã€\n]", raw_points) if p.strip()]
                                else:
                                    points_list = [str(p).strip() for p in (raw_points or []) if str(p).strip()]
                                question_text = q.get("é¢è¯•é¢˜", "")
                                if isinstance(question_text, list):
                                    question_text = "ï¼›".join(str(item).strip() for item in question_text if str(item).strip())
                                qs.append({
                                    "dimension": q.get("ç»´åº¦åç§°", ""),
                                    "question": question_text,
                                    "points": points_list,
                                    "score": float(q.get("åˆ†å€¼", 0)),
                                })
                            if qs:
                                bundle["interview"] = qs
                            bundle["full_ability_list"] = construct_full_ability_list(
                                bundle.get("dimensions"), bundle.get("interview")
                            )
                    # âœ… æŒä¹…åŒ–ï¼šåç»­å…¶å®ƒæŒ‰é’®/åŒºåŸŸå¯å¤ç”¨
                    st.session_state["ai_bundle"] = bundle
                    st.success("âœ… AI ç”Ÿæˆå®Œæˆ")
                except Exception as e:
                    error_msg = str(e)
                    # æå–æ›´å‹å¥½çš„é”™è¯¯ä¿¡æ¯
                    if "Key" in error_msg or "æœªé…ç½®" in error_msg:
                        st.error(f"âŒ {error_msg}")
                        st.info("ğŸ’¡ è¯·æ£€æŸ¥é¡¹ç›®æ ¹ç›®å½•çš„ `.env` æ–‡ä»¶ï¼Œç¡®ä¿åŒ…å« SILICONFLOW_API_KEY æˆ– OPENAI_API_KEYï¼Œç„¶åé‡å¯ Streamlitã€‚")
                    elif "401" in error_msg or "403" in error_msg:
                        st.error(f"âŒ API Key éªŒè¯å¤±è´¥ï¼š{error_msg}")
                        st.info("ğŸ’¡ è¯·æ£€æŸ¥ .env æ–‡ä»¶ä¸­çš„ API Key æ˜¯å¦æ­£ç¡®ï¼Œæˆ–æ˜¯å¦å·²è¿‡æœŸã€‚")
                    elif "404" in error_msg or "æ¨¡å‹" in error_msg:
                        st.error(f"âŒ æ¨¡å‹ä¸å¯ç”¨ï¼š{error_msg}")
                        st.info("ğŸ’¡ è¯·æ£€æŸ¥ .env æ–‡ä»¶ä¸­çš„ AI_MODEL æ˜¯å¦æ­£ç¡®ï¼Œæˆ–å°è¯•æ›´æ¢ä¸ºå…¶ä»–å¯ç”¨æ¨¡å‹ï¼ˆå¦‚ Qwen2.5-32B-Instructï¼‰ã€‚")
                    else:
                        st.error(f"âŒ AI ç”Ÿæˆå¤±è´¥ï¼š{error_msg}")
                        st.info("ğŸ’¡ ç³»ç»Ÿå°†ç»§ç»­æ”¯æŒ'ç¦»çº¿è§„åˆ™ç‰ˆ'ç”Ÿæˆï¼Œç¡®ä¿å¯ç”¨ã€‚å±•å¼€ä¸‹æ–¹çš„'AI è¿æ¥è¯Šæ–­'æŸ¥çœ‹è¯¦ç»†é”™è¯¯ä¿¡æ¯ã€‚")
    
    # æ˜¾ç¤ºAIç”Ÿæˆç»“æœ
    bundle = st.session_state.get("ai_bundle")
    if SHOW_DETAIL_SECTIONS:
        if bundle:
            st.subheader("ğŸ“„ é•¿ç‰ˆ JDï¼ˆBossç›´è˜å¯ç”¨ï¼‰")
            st.text_area("é•¿ç‰ˆ JD", bundle["jd_long"], height=260)
        
            st.subheader("ğŸª§ çŸ­ç‰ˆ JDï¼ˆç¤¾åª’/å†…æ¨ï¼‰")
            st.text_area("çŸ­ç‰ˆ JD", bundle["jd_short"], height=100)
        
            st.markdown("### å²—ä½èƒ½åŠ›ç»´åº¦ä¸é¢è¯•é¢˜ç›®ï¼ˆAIåˆ†æ + AIç”Ÿæˆï¼‰")
            full_ability = bundle.get("full_ability_list") or construct_full_ability_list(
                bundle.get("dimensions"), bundle.get("interview")
            )
            bundle["full_ability_list"] = full_ability

            display_rows = []
            for item in full_ability:
                display_rows.append({
                    "èƒ½åŠ›ç»´åº¦": item.get("dimension", ""),
                    "è¯´æ˜": item.get("description", ""),
                    "æƒé‡(%)": round(float(item.get("weight", 0.0)) * 100, 1),
                    "é¢è¯•é¢˜ç›®": item.get("question", ""),
                    "è¯„åˆ†è¦ç‚¹": item.get("score_points", ""),
                    "20åˆ†è¡Œä¸ºè¡¨ç°": item.get("score_20", ""),
                    "60åˆ†è¡Œä¸ºè¡¨ç°": item.get("score_60", ""),
                    "100åˆ†è¡Œä¸ºè¡¨ç°": item.get("score_100", ""),
                    "åˆ†å€¼": item.get("score_value", 0.0),
                })

            df_full = pd.DataFrame(display_rows)
            st.dataframe(df_full, use_container_width=True)

            # ä½¿ç”¨æ¨¡æ¿ç”Ÿæˆ Excelï¼ˆæ–°ç‰ˆæœ¬ï¼Œå®Œå…¨åŸºäºæ¨¡æ¿ï¼‰
            job_name = (st.session_state.get('job_name') or 'å²—ä½').strip()
            try:
                # è½¬æ¢æ•°æ®æ ¼å¼ä¸º DataFrame
                dimensions_data = []
                for ability in full_ability:
                    dimensions_data.append({
                        "èƒ½åŠ›ç»´åº¦": ability.get("dimension", ""),
                        "è¯´æ˜": ability.get("description", ""),
                        "é¢è¯•é¢˜ç›®": ability.get("question", ""),
                        "è¯„åˆ†è¦ç‚¹": ability.get("score_points", ""),
                        "20åˆ†è¡Œä¸ºè¡¨ç°": ability.get("score_20", ""),
                        "60åˆ†è¡Œä¸ºè¡¨ç°": ability.get("score_60", ""),
                        "100åˆ†è¡Œä¸ºè¡¨ç°": ability.get("score_100", ""),
                        "æƒé‡": ability.get("weight", 0.0),
                    })
                
                # åˆ›å»º DataFrameï¼ˆå»æ‰æ§åˆ¶å°è°ƒè¯•è¾“å‡ºï¼Œé¿å… Windows æ§åˆ¶å°ç¼–ç å¯¼è‡´ OSErrorï¼‰
                data_df = pd.DataFrame(dimensions_data)
                
                # å›ºå®šè¾“å‡ºè·¯å¾„
                output_path = r"C:\RecruitFlow_Pro_MVP\docs\è¯¾ç¨‹é¡¾é—®_èƒ½åŠ›ç»´åº¦è¯„åˆ†è¡¨(æ”¹)_è¾“å‡º.xlsx"
                
                def _coerce_excel_result(result, fallback_path):
                    if isinstance(result, tuple):
                        return result
                    if isinstance(result, bytes):
                        return result, fallback_path
                    read_path = result if isinstance(result, str) else fallback_path
                    with open(read_path, "rb") as f:
                        return f.read(), read_path
                
                # ä½¿ç”¨æ–°çš„å¯¼å‡ºå‡½æ•°ï¼ˆå®Œå…¨åŸºäºæ¨¡æ¿ï¼‰
                try:
                    export_result = export_competency_excel(
                        data_df, output_path, job_title=job_name
                    )
                except TypeError:
                    print("[streamlit] export_competency_excel fallback to legacy signature")
                    export_result = export_competency_excel(data_df, output_path)

                excel_bytes, saved_path = _coerce_excel_result(export_result, output_path)

                if saved_path and saved_path != output_path:
                    st.warning(f"åŸå§‹è¾“å‡ºæ–‡ä»¶è¢«å ç”¨ï¼Œå·²æ”¹ä¸ºä¿å­˜åˆ°ï¼š`{saved_path}`")
                
                download_name = f"{job_name}_èƒ½åŠ›ç»´åº¦è¯„åˆ†è¡¨.xlsx"
                st.download_button(
                    "ğŸ“„ å¯¼å‡ºèƒ½åŠ›ç»´åº¦è¯„åˆ†è¡¨ï¼ˆExcelï¼‰",
                    data=excel_bytes,
                    file_name=download_name,
                    mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                    use_container_width=True,
                )
            except Exception as e:
                st.error(f"å¯¼å‡ºå¤±è´¥ï¼š{str(e)}")
                st.exception(e)
 
            # ä¿ç•™å•ä¸€ä¿å­˜å…¥å£
            if st.button("ğŸ’¾ å†™å…¥ç³»ç»Ÿï¼ˆä¿å­˜ JD + é¢˜åº“ï¼‰", type="primary", key="btn_save_rubric_1"):
                save_to_system_action()
        else:
            st.info('å°šæœªç”Ÿæˆ Rubricï¼ˆè¯·å…ˆç‚¹å‡»ä¸Šæ–¹"ç”Ÿæˆ JD"ï¼‰')
        
        # âœ… éšè—è¯„åˆ†ç»´åº¦è§„åˆ™ï¼ˆRubricï¼‰éƒ¨åˆ†ï¼Œåªä¿ç•™åŠŸèƒ½é€»è¾‘
        # è¿™é‡Œä¿ç•™ bundle_for_rubric çš„ç”Ÿæˆå’Œä¿å­˜é€»è¾‘ï¼Œä½†ä¸æ¸²æŸ“åˆ°é¡µé¢
        bundle_for_rubric = st.session_state.get("ai_bundle")
        # bundle_for_rubric å˜é‡ä¿ç•™ï¼Œä¾›å†…éƒ¨é€»è¾‘ä½¿ç”¨ï¼ˆå¦‚ save_to_system_action å‡½æ•°ä¸­ä¼šç”¨åˆ°ï¼‰
        
        # ä¸å†æ˜¾ç¤ºæ ‡é¢˜å’Œå±•å¼€å—ï¼Œé¿å… UI é‡å¤
        # st.subheader("è¯„åˆ†ç»´åº¦è§„åˆ™ï¼ˆRubricï¼‰")  # âŒ æ³¨é‡Šæ‰
        # with st.expander("è¯„åˆ†ç»´åº¦è§„åˆ™ï¼ˆRubricï¼‰", expanded=False):
        #     st.json(bundle_for_rubric["rubric"])
        
        # âœ… ä»…ä¿ç•™ä¸€æ¬¡ä¿å­˜æŒ‰é’®ï¼ˆä¸Šæ–¹å·²æœ‰çš„æŒ‰é’® btn_save_rubric_1ï¼‰
        # å› æ­¤è¿™é‡Œåˆ é™¤é‡å¤æŒ‰é’®ï¼Œé˜²æ­¢é‡å¤æ˜¾ç¤º
        # if st.button("ğŸ’¾ å†™å…¥ç³»ç»Ÿï¼ˆä¿å­˜ JD + é¢˜åº“ï¼‰", type="primary", key="btn_save_rubric_2"):
        #     save_to_system_action()
    
    # ==== AI è¿æ¥è¯Šæ–­ï¼ˆæ”¾åœ¨é¡µé¢åº•éƒ¨ï¼‰====
    with st.expander("ğŸ”§ AI è¿æ¥è¯Šæ–­ï¼ˆæ‰“ä¸å¼€å°±ç‚¹æˆ‘ï¼‰"):
        try:
            # å¼ºåˆ¶é‡æ–°åŠ è½½æ¨¡å—ï¼Œé¿å…ç¼“å­˜é—®é¢˜
            import importlib
            import sys
            if 'backend.services.ai_client' in sys.modules:
                importlib.reload(sys.modules['backend.services.ai_client'])
            from backend.services.ai_client import get_client_and_cfg, AIConfig, chat_completion
        except ImportError as e:
            st.error(f"âŒ å¯¼å…¥ AI å®¢æˆ·ç«¯å¤±è´¥ï¼š{e}")
            st.info("ğŸ’¡ è¯·æ£€æŸ¥ backend/services/ai_client.py æ–‡ä»¶æ˜¯å¦å­˜åœ¨ä¸”å¯æ­£å¸¸å¯¼å…¥")
            st.stop()
        
        cfg = AIConfig()
        key_present = bool(cfg.api_key)
        st.write("**å·²æ£€æµ‹åˆ° Keyï¼š**", "âœ…" if key_present else "âŒ")
        if key_present:
            st.write("**Key å‰ç¼€ï¼š**", cfg.api_key[:10] + "..." if len(cfg.api_key) > 10 else cfg.api_key)
        st.write("**Base URLï¼š**", cfg.base_url)
        st.write("**å½“å‰æ¨¡å‹ï¼š**", cfg.model)
        st.write("**Temperatureï¼š**", cfg.temperature)
        
        if st.button("ğŸ§ª æµ‹è¯•ä¸€æ¬¡ AI è¿é€šæ€§"):
            try:
                client, cfg = get_client_and_cfg()
                with st.spinner("æ­£åœ¨æµ‹è¯•è¿æ¥..."):
                    res = chat_completion(
                        client,
                        cfg,
                        messages=[{"role":"user","content":"åªè¿”å› OK"}],
                        temperature=0,
                        max_tokens=10
                    )
                    result = res["choices"][0]["message"]["content"].strip()
                    st.success(f"âœ… AI è¿é€šæ€§æµ‹è¯•æˆåŠŸï¼è¿”å›ï¼š{result}")
            except Exception as e:
                error_detail = str(e)
                st.error(f"âŒ è¿é€šæ€§å¤±è´¥ï¼š{error_detail}")
                if "ChatCompletion" in error_detail or "openai>=1.0.0" in error_detail:
                    st.error("âš ï¸ OpenAI API ç‰ˆæœ¬å…¼å®¹æ€§é—®é¢˜")
                    st.info("ğŸ’¡ è¿™é€šå¸¸æ˜¯å› ä¸ºä»£ç ä¸­ä½¿ç”¨äº†æ—§ç‰ˆæœ¬çš„ OpenAI APIã€‚è¯·ç¡®ä¿ï¼š\n"
                           "1. å·²å®‰è£… openai>=1.0.0ï¼š`pip install --upgrade openai`\n"
                           "2. ä»£ç ä½¿ç”¨ `client.chat.completions.create` è€Œä¸æ˜¯ `openai.ChatCompletion.create`\n"
                           "3. é‡å¯ Streamlit åº”ç”¨ä»¥æ¸…é™¤ç¼“å­˜")
                    st.code("pip install --upgrade openai", language="bash")
                elif "Key" in error_detail or "æœªé…ç½®" in error_detail:
                    st.info("ğŸ’¡ æ£€æŸ¥ .env çš„ Key é…ç½®ï¼›ç¡®ä¿æ–‡ä»¶åœ¨é¡¹ç›®æ ¹ç›®å½•ï¼›é‡å¯ Streamlit")
                elif "401" in error_detail or "403" in error_detail:
                    st.info("ğŸ’¡ API Key æ— æ•ˆæˆ–å·²è¿‡æœŸï¼Œè¯·æ£€æŸ¥ .env ä¸­çš„ Key æ˜¯å¦æ­£ç¡®")
                elif "404" in error_detail:
                    st.info("ğŸ’¡ æ¨¡å‹ä¸å­˜åœ¨æˆ–æœªå¼€é€šï¼Œè¯·æ£€æŸ¥ .env ä¸­çš„ AI_MODELï¼Œå°è¯•æ›´æ¢ä¸º Qwen2.5-32B-Instruct")
                elif "timeout" in error_detail.lower() or "è¿æ¥" in error_detail:
                    st.info("ğŸ’¡ ç½‘ç»œè¿æ¥é—®é¢˜ï¼Œæ£€æŸ¥å…¬å¸ç½‘ç»œæ˜¯å¦æ”¾è¡Œ api.siliconflow.cnï¼›æˆ–å°è¯•ä½¿ç”¨ OpenAI")
                else:
                    st.info("ğŸ’¡ æ£€æŸ¥ .env çš„ Key/æ¨¡å‹/Base URLï¼›æˆ–å…¬å¸ç½‘ç»œæ˜¯å¦æ”¾è¡Œ api.siliconflow.cn")
    
    # ä¸€é”®å¯åŠ¨è¯´æ˜
    with st.expander("ğŸš€ ä¸€é”®å¯åŠ¨ç¨‹åºï¼ˆé¦–æ¬¡ä½¿ç”¨å¿…çœ‹ï¼‰", expanded=False):
        st.markdown("""
        ### å¿«é€Ÿå¯åŠ¨æ–¹æ³•
        
        1. **æœ€ç®€å•æ–¹å¼**ï¼šåŒå‡»é¡¹ç›®æ ¹ç›®å½•çš„ `å¯åŠ¨ç¨‹åº.bat` æ–‡ä»¶
        2. **PowerShell æ–¹å¼**ï¼šå³é”® `å¯åŠ¨ç¨‹åº.ps1` -> ä½¿ç”¨ PowerShell è¿è¡Œ
        3. **å‘½ä»¤è¡Œæ–¹å¼**ï¼šè¿è¡Œ `å¯åŠ¨ç¨‹åº.bat` æˆ– `.\\å¯åŠ¨ç¨‹åº.ps1`
        
        ### é¦–æ¬¡ä½¿ç”¨å‰å‡†å¤‡
        
        - âœ… ç¡®ä¿å·²å®‰è£… Python 3.8+
        - âœ… å·²åˆ›å»ºè™šæ‹Ÿç¯å¢ƒï¼š`python -m venv .venv`
        - âœ… å·²å®‰è£…ä¾èµ–ï¼š`.venv\\Scripts\\pip install -r requirements.txt`
        - âœ… å·²é…ç½® `.env` æ–‡ä»¶ï¼ˆAI Key ç­‰ï¼Œå¯é€‰ï¼‰
        
        ### è¯¦ç»†ä½¿ç”¨è¯´æ˜
        
        è¯·æŸ¥çœ‹é¡¹ç›®æ ¹ç›®å½•çš„ `ä½¿ç”¨è¯´æ˜.md` æ–‡ä»¶ï¼ŒåŒ…å«ï¼š
        - ğŸ“‹ å®Œæ•´åŠŸèƒ½è¯´æ˜
        - ğŸ”§ å¸¸è§é—®é¢˜è§£ç­”
        - ğŸ¯ å„åŠŸèƒ½æ¨¡å—ä½¿ç”¨æŒ‡å—
        
        ### å½“å‰è¿è¡ŒçŠ¶æ€
        
        - ğŸŒ è®¿é—®åœ°å€ï¼šhttp://localhost:8501
        - ğŸ“ é¡¹ç›®ç›®å½•ï¼š""" + str(Path.cwd()) + """
        """)
        
        # æ˜¾ç¤ºå¯åŠ¨è„šæœ¬è·¯å¾„
        bat_path = Path.cwd() / "å¯åŠ¨ç¨‹åº.bat"
        ps1_path = Path.cwd() / "å¯åŠ¨ç¨‹åº.ps1"
        
        if bat_path.exists():
            st.success(f"âœ… å¯åŠ¨è„šæœ¬å·²æ‰¾åˆ°ï¼š`{bat_path}`")
        else:
            st.warning(f"âš ï¸ å¯åŠ¨è„šæœ¬ä¸å­˜åœ¨ï¼š`{bat_path}`")
        
        if ps1_path.exists():
            st.success(f"âœ… PowerShell è„šæœ¬å·²æ‰¾åˆ°ï¼š`{ps1_path}`")
        
        # æä¾›å¿«é€Ÿå‘½ä»¤
        cmd_text = f"""# å¿«é€Ÿå¯åŠ¨å‘½ä»¤ï¼ˆå¤åˆ¶åˆ°å‘½ä»¤è¡Œè¿è¡Œï¼‰
cd "{Path.cwd()}"
.venv\\Scripts\\python.exe -m streamlit run app/streamlit_app.py --server.port 8501"""
        st.code(cmd_text, language="bash")
    
    st.markdown("---")
    if SHOW_OFFLINE_SECTION:
        st.markdown("---")
        st.markdown("### ğŸ“‹ ç¦»çº¿è§„åˆ™ç‰ˆï¼ˆå¤‡ç”¨ï¼‰")
        
        # é‡æ–°è¯»å–é…ç½®ï¼ˆå› ä¸ºå¯èƒ½åœ¨ä¾§è¾¹æ å·²æ›´æ–°ï¼‰
        cfg = json.loads(cfg_file.read_text(encoding="utf-8"))
        use_ai = cfg.get("llm_provider") in ["openai", "claude", "siliconflow"]
        
        # è¾“å…¥è¡¨å•ï¼ˆç¦»çº¿ç‰ˆï¼‰
        with st.form("jd_generation_form"):
            col1, col2 = st.columns(2)
            with col1:
                job_name = st.text_input("å²—ä½åç§° *", placeholder="ä¾‹å¦‚ï¼šæ•°æ®åˆ†æå¸ˆã€äº§å“ç»ç†ã€è¿è¥ä¸“å‘˜ç­‰", 
                                        value=st.session_state.get("job_name", ""))
            with col2:
                st.caption("ğŸ’¡ å¿…å¡«é¡¹")
            
            must_have = st.text_area("å¿…å¤‡ç»éªŒ/æŠ€èƒ½", placeholder="ä¾‹å¦‚ï¼š3å¹´ä»¥ä¸Šæ•°æ®åˆ†æç»éªŒï¼›ç†Ÿæ‚‰Pythonã€SQLï¼›æœ‰æ•™è‚²è¡Œä¸šèƒŒæ™¯", 
                                    height=80, help="ç”¨åˆ†å·(;)åˆ†éš”å¤šä¸ªæŠ€èƒ½")
            nice_to_have = st.text_area("åŠ åˆ†é¡¹", placeholder="ä¾‹å¦‚ï¼šç†Ÿæ‚‰æœºå™¨å­¦ä¹ ï¼›æœ‰å›¢é˜Ÿç®¡ç†ç»éªŒï¼›æ•°æ®å¯è§†åŒ–èƒ½åŠ›å¼º", 
                                       height=80, help="ç”¨åˆ†å·(;)åˆ†éš”å¤šä¸ªåŠ åˆ†é¡¹")
            exclude_keywords = st.text_area("æ’é™¤é¡¹", placeholder="ä¾‹å¦‚ï¼šé¢‘ç¹è·³æ§½ï¼›ä»…å®ä¹ ç»éªŒï¼›å¤–åŒ…ç»å†", 
                                           height=60, help="ç”¨åˆ†å·(;)åˆ†éš”å¤šä¸ªæ’é™¤å…³é”®è¯")
            
            submitted = st.form_submit_button("ğŸš€ ç”Ÿæˆ JD", type="primary", use_container_width=True)
        
        # å¤„ç†ç”Ÿæˆè¯·æ±‚
        if submitted:
            if not job_name:
                st.error("âŒ è¯·å¡«å†™å²—ä½åç§°")
            else:
                st.session_state["job_name"] = job_name
                with st.spinner("ğŸ¤– AIæ­£åœ¨æ™ºèƒ½åˆ†æå²—ä½éœ€æ±‚ï¼Œç”Ÿæˆä¸“ä¸šJDã€èƒ½åŠ›ç»´åº¦ã€é¢è¯•é¢˜ç›®ï¼Œè¯·ç¨å€™ï¼ˆé€šå¸¸éœ€è¦10-30ç§’ï¼‰..."):
                    try:
                        jd_long, jd_short, rubric, interview_questions = pipe.generate_jd(
                            job_name, must_have=must_have, nice_to_have=nice_to_have, 
                            exclude_keywords=exclude_keywords, use_ai=use_ai
                        )
                        st.session_state["jd_result"] = (jd_long, jd_short, rubric, interview_questions)
                        st.success("âœ… AIç”ŸæˆæˆåŠŸï¼")
                    except Exception as e:
                        st.error(f"âŒ ç”Ÿæˆå¤±è´¥: {str(e)}")
                        if use_ai:
                            st.info("æ­£åœ¨å°è¯•ä½¿ç”¨ç¦»çº¿æ¨¡å¼...")
                            try:
                                jd_long, jd_short, rubric, interview_questions = pipe.generate_jd(
                                    job_name, must_have=must_have, nice_to_have=nice_to_have, 
                                    exclude_keywords=exclude_keywords, use_ai=False
                                )
                                st.session_state["jd_result"] = (jd_long, jd_short, rubric, interview_questions)
                                st.success("âœ… ç¦»çº¿æ¨¡å¼ç”ŸæˆæˆåŠŸ")
                            except Exception as e2:
                                st.error(f"âŒ ç¦»çº¿æ¨¡å¼ä¹Ÿå¤±è´¥: {str(e2)}")
        
        # æ˜¾ç¤ºç»“æœ
        if "jd_result" in st.session_state:
            jd_long, jd_short, rubric, interview_questions = st.session_state["jd_result"]
            
            # é•¿ç‰ˆJD
            st.markdown("### ğŸ“„ é•¿ç‰ˆ JDï¼ˆBossç›´è˜å¯ç”¨ï¼‰")
            st.text_area("", jd_long, height=300, key="jd_long_display", label_visibility="collapsed")
            
            # çŸ­ç‰ˆJD
            st.markdown("### âœ¨ çŸ­ç‰ˆ JDï¼ˆç¤¾åª’/å†…æ¨ï¼‰")
            st.text_area("", jd_short, height=100, key="jd_short_display", label_visibility="collapsed")
            
            # èƒ½åŠ›ç»´åº¦
            st.markdown("### ğŸ¯ å²—ä½èƒ½åŠ›ç»´åº¦ï¼ˆAIåˆ†æï¼‰")
            if rubric.get("dimensions"):
                dim_data = []
                for dim in rubric["dimensions"]:
                    weight = float(dim.get("weight", 0))
                    dim_data.append({
                        "èƒ½åŠ›ç»´åº¦": dim.get("name", ""),
                        "æƒé‡": f"{weight * 100:.1f}%",
                        "è¯´æ˜": dim.get("description", "")
                    })
                dim_df = pd.DataFrame(dim_data)
                st.dataframe(dim_df, use_container_width=True)
            else:
                st.info('å°šæœªç”Ÿæˆ Rubricï¼ˆè¯·å…ˆç‚¹å‡»ä¸Šæ–¹"ç”Ÿæˆ JD"ï¼‰')
            
            # é¢è¯•é¢˜ç›®
            st.markdown("### ğŸ’¬ é¢è¯•é¢˜ç›®å’Œè¯„åˆ†æ ‡å‡†ï¼ˆAIç”Ÿæˆï¼‰")
            if interview_questions and interview_questions.get("questions"):
                for idx, q in enumerate(interview_questions["questions"], 1):
                    weight_pct = float(q.get('weight', 0)) * 100
                    with st.expander(f"é¢˜ç›® {idx}: {q.get('dimension', 'é€šç”¨')} - æƒé‡: {weight_pct:.0f}%"):
                        st.markdown(f"**é—®é¢˜ï¼š** {q.get('question', '')}")
                        st.markdown(f"**è¯„åˆ†æ ‡å‡†ï¼š** {q.get('evaluation_criteria', '')}")
                        if q.get('weight'):
                            st.caption(f"æƒé‡: {float(q.get('weight', 0)) * 100:.0f}%")
            else:
                st.info("æš‚æ— é¢è¯•é¢˜ç›®")
            
            # ä¿å­˜æŒ‰é’®
            col1, col2 = st.columns([1, 4])
            with col1:
                if st.button("ğŸ’¾ ä¿å­˜ JD & è¯„åˆ†ç»´åº¦", type="primary", key="btn_save_jd_score"):
                    pipe.save_jd(job_name, jd_long, jd_short, rubric, interview_questions)
                    st.success("âœ… å·²ä¿å­˜")

with tab2:
    st.subheader("å¯¼å…¥ç®€å†ï¼ˆCSV/TXT ç¤ºä¾‹ï¼‰å¹¶åŒ¹é…æ‰“åˆ†")
    uploaded = st.file_uploader("ä¸Šä¼ ç®€å† CSVï¼ˆè§ data/samples/sample_resumes.csvï¼‰æˆ– TXTï¼ˆå•ä¸ªï¼‰", type=["csv","txt"], accept_multiple_files=True)
    if uploaded:
        for f in uploaded:
            if f.name.endswith(".csv"):
                df = pd.read_csv(f); pipe.ingest_resumes_df(df)
            else:
                txt = f.read().decode("utf-8", errors="ignore"); pipe.ingest_text_resume(txt)
        st.success("å·²å¯¼å…¥")
    if st.button("æ‰¹é‡è¯„åˆ†"):
        start = time.time()
        result_df = pipe.score_all(st.session_state.get("job_name"))
        st.session_state["scored"] = result_df
        st.info(f"è¯„åˆ†å®Œæˆï¼Œç”¨æ—¶ {time.time()-start:.2f} s")
        # æ±‰åŒ–æ˜¾ç¤º
        result_df_display = translate_dataframe_columns(result_df)
        st.dataframe(result_df_display, use_container_width=True)

    st.markdown("---")
    st.markdown("## ğŸ¤– AI æ™ºèƒ½åŒ¹é…ï¼ˆæ‰¹é‡ä¸Šä¼  PDF/DOCX/å›¾ç‰‡ï¼‰")

    jd_text = ""
    if st.session_state.get("ai_bundle") and st.session_state["ai_bundle"].get("jd_long"):
        jd_text = st.session_state["ai_bundle"]["jd_long"]

    jd_text = st.text_area(
        "å²—ä½ JD æ–‡æœ¬ï¼ˆå·²è‡ªåŠ¨å¸¦å…¥ AI é•¿ç‰ˆ JDï¼Œå¯æ‰‹åŠ¨ç¼–è¾‘ï¼‰",
        value=jd_text,
        height=200,
        help="AI ä¼šåŸºäºè¿™é‡Œçš„ JD ä¸ç®€å†è¿›è¡ŒåŒ¹é…ï¼Œè¯·ç¡®ä¿å†…å®¹å‡†ç¡®ã€‚"
    )

    uploaded_files = st.file_uploader(
        "ä¸Šä¼ å¤šä»½ç®€å†ï¼ˆæ”¯æŒï¼špdfã€docxã€txtã€jpgã€jpegã€pngï¼‰",
        type=["pdf", "docx", "txt", "jpg", "jpeg", "png"],
        accept_multiple_files=True,
        key="ai_resume_uploader"
    )

    if uploaded_files:
        with st.spinner("æ­£åœ¨è§£æç®€å†æ–‡ä»¶â€¦"):
            resumes_df = parse_uploaded_files_to_df(uploaded_files)
        if resumes_df.empty:
            st.warning("æ²¡æœ‰è§£æåˆ°æœ‰æ•ˆç®€å†ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶æ ¼å¼ã€‚")
        else:
            st.success(f"å·²è§£æ {len(resumes_df)} ä»½ç®€å†ã€‚")
            base_columns = ["candidate_id", "name", "file", "email", "phone", "text_len"]
            for col in base_columns:
                if col not in resumes_df.columns:
                    if col == "candidate_id":
                        resumes_df[col] = range(1, len(resumes_df) + 1)
                    elif col == "text_len":
                        resumes_df[col] = resumes_df.get("resume_text", "").apply(lambda x: len(str(x)) if x else 0)
                    else:
                        resumes_df[col] = ""
            # ä½¿ç”¨å­—æ®µæ˜ å°„ç¿»è¯‘åˆ—å
            display_resumes_df = resumes_df[base_columns].copy()
            display_resumes_df = translate_dataframe_columns(display_resumes_df)
            st.dataframe(
                display_resumes_df,
                use_container_width=True
            )

            if st.button("ğŸš€ ç”¨ AI æ‰¹é‡åŒ¹é…å¹¶æ‰“åˆ†"):
                if not jd_text.strip():
                    st.warning("è¯·å…ˆå¡«å†™/ç²˜è´´å²—ä½ JDã€‚")
                else:
                    # è·å–å²—ä½åç§°ï¼Œç”¨äºå²—ä½çº§æ¸…æ´—é€»è¾‘
                    job_title = st.session_state.get("job_name", "")
                    with st.spinner("AI æ­£åœ¨æ™ºèƒ½åˆ†æåŒ¹é…åº¦ï¼Œè¯·ç¨å€™â€¦"):
                        scored_df = ai_match_resumes_df(jd_text, resumes_df, job_title)
                    score_columns = [
                        "candidate_id",
                        "name",
                        "file",
                        "email",
                        "phone",
                        "æ€»åˆ†",
                        "æŠ€èƒ½åŒ¹é…åº¦",
                        "ç»éªŒç›¸å…³æ€§",
                        "æˆé•¿æ½œåŠ›",
                        "ç¨³å®šæ€§",
                        "score_explain",
                        "short_eval",
                        "highlights",
                        "resume_mini",
                        "è¯æ®",
                    ]
                    for col in score_columns:
                        if col not in scored_df.columns:
                            if col == "candidate_id":
                                scored_df[col] = range(1, len(scored_df) + 1)
                            else:
                                scored_df[col] = ""
                    
                    result_df = scored_df
                    display_columns = [
                        "candidate_id",
                        "name",
                        "file",
                        "æ€»åˆ†",
                        "æŠ€èƒ½åŒ¹é…åº¦",
                        "ç»éªŒç›¸å…³æ€§",
                        "æˆé•¿æ½œåŠ›",
                        "ç¨³å®šæ€§",
                        "short_eval",
                        "highlights",
                        "resume_mini",
                        "è¯æ®",
                    ]
                    existing_display = [col for col in display_columns if col in result_df.columns]
                    if existing_display:
                        display_df = result_df[existing_display].copy()
                        if "resume_mini" in display_df.columns:
                            display_df["resume_mini"] = display_df["resume_mini"].apply(
                                lambda x: (x[:80] + "â€¦") if isinstance(x, str) and len(x) > 80 else x
                            )
                        display_df = translate_dataframe_columns(display_df)
                        st.dataframe(
                            display_df,
                            use_container_width=True,
                            hide_index=True,
                        )
                    export_job_title = st.session_state.get("job_name") or job_title or "æœªæä¾›"
                    export_df = _build_export_dataframe(result_df, export_job_title)

                    st.markdown("### å€™é€‰äººæ´å¯Ÿè¯¦æƒ…")
                    for _, row in result_df.iterrows():
                        score_label = row.get("æ€»åˆ†")
                        title = f"{row.get('name','åŒ¿åå€™é€‰äºº')}ï½œæ€»åˆ† {score_label if score_label is not None else 'â€”'}"
                        with st.expander(title):
                            raw_highlights = row.get("highlights", "")
                            if isinstance(raw_highlights, str):
                                highlights_raw = [tag.strip() for tag in re.split(r"[ï½œ|ï¼Œ,ã€\s]+", raw_highlights) if tag.strip()]
                            elif isinstance(raw_highlights, list):
                                highlights_raw = raw_highlights
                            else:
                                highlights_raw = []
                            if highlights_raw:
                                st.markdown(
                                    "**äº®ç‚¹æ ‡ç­¾**ï¼š" + " ".join(f"`{tag}`" for tag in highlights_raw if tag)
                                )
                            else:
                                st.markdown("**äº®ç‚¹æ ‡ç­¾**ï¼šæš‚æ— ")

                            resume_mini = row.get("resume_mini", "")
                            st.markdown("**çŸ­ç‰ˆç®€å†**")
                            st.write(resume_mini if resume_mini else "æš‚æ— çŸ­ç‰ˆç®€å†")

                            st.markdown("**AI æ¨ç†é“¾**")
                            reasoning_raw = row.get("reasoning_chain") or {}
                            try:
                                reasoning_obj = (
                                    json.loads(reasoning_raw)
                                    if isinstance(reasoning_raw, str)
                                    else reasoning_raw
                                )
                            except Exception:
                                reasoning_obj = {}
                            def render_chain(title: str, chain: list, fields: list):
                                st.markdown(f"##### {title}")
                                if not chain:
                                    st.caption("æš‚æ— ç›¸å…³è®°å½•")
                                    return
                                for idx, item in enumerate(chain, 1):
                                    if not isinstance(item, dict):
                                        continue
                                    st.markdown(f"**{idx}. {item.get('conclusion', 'æ— ç»“è®º')}**")
                                    for label, key in fields:
                                        value = str(item.get(key, "")).strip()
                                        if value:
                                            st.markdown(f"- {label}ï¼š{value}")
                                    st.markdown("---")
                            strengths_chain = reasoning_obj.get("strengths_reasoning_chain") or []
                            weaknesses_chain = reasoning_obj.get("weaknesses_reasoning_chain") or []
                            render_chain(
                                "ä¼˜åŠ¿æ¨ç†é“¾",
                                strengths_chain,
                                [
                                    ("detected_actions", "detected_actions"),
                                    ("resume_evidence", "resume_evidence"),
                                    ("ai_reasoning", "ai_reasoning"),
                                ],
                            )
                            render_chain(
                                "åŠ£åŠ¿æ¨ç†é“¾",
                                weaknesses_chain,
                                [
                                    ("resume_gap", "resume_gap"),
                                    ("compare_to_jd", "compare_to_jd"),
                                    ("ai_reasoning", "ai_reasoning"),
                                ],
                            )

                    # âœ… ä¸€é”®ä¿®å¤ç‰ˆï¼šAI åŒ¹é…å®Œæˆåè‡ªåŠ¨ä¿å­˜ & è·³è½¬

                    # åˆ¤æ–­AIåŒ¹é…ç»“æœæ˜¯å¦ä¸ºç©º
                    if "result_df" in locals() and not result_df.empty:
                        # ä¿å­˜è¯„åˆ†ç»“æœåˆ°session_stateï¼Œä¾›ä¸‹ä¸€æ­¥â€œå»é‡&æ’åºâ€ä½¿ç”¨
                        st.session_state["score_df"] = result_df
                        st.session_state["scored"] = result_df

                        # æ˜¾ç¤ºæˆåŠŸæç¤º
                        st.success("AI åŒ¹é…åˆ†æå®Œæˆ âœ…")
                        st.info("ç³»ç»Ÿå·²è‡ªåŠ¨ä¿å­˜è¯„åˆ†ç»“æœï¼Œè¯·ç‚¹å‡»é¡¶éƒ¨å¯¼èˆªæ ã€3 å»é‡ & æ’åºã€æŸ¥çœ‹ Top-N å€™é€‰äººã€‚")

                        # è‡ªåŠ¨å¯¼å‡ºCSVæ–‡ä»¶åˆ°é¡¹ç›®dataç›®å½•
                        import os
                        output_path = os.path.join("data", "ai_match_results.csv")
                        try:
                            export_df.to_csv(output_path, index=False, encoding="utf-8-sig")
                            st.write(f"âœ… å·²è‡ªåŠ¨ä¿å­˜åŒ¹é…ç»“æœè‡³ `{output_path}`")
                        except Exception as e:
                            st.warning(f"âš ï¸ ä¿å­˜CSVå¤±è´¥: {e}")

                        # ï¼ˆå¯é€‰ï¼‰æä¾›ä¸‹è½½æŒ‰é’®
                        st.download_button(
                            label="â¬‡ï¸ ä¸‹è½½ AI åŒ¹é…ç»“æœï¼ˆCSVï¼‰",
                            data=export_df.to_csv(index=False).encode("utf-8-sig"),
                            file_name="ai_match_results.csv",
                            mime="text/csv"
                        )
                    else:
                        st.warning("âš ï¸ æš‚æ— åŒ¹é…ç»“æœï¼Œè¯·å…ˆå®ŒæˆAIåŒ¹é…è¯„åˆ†åå†å°è¯•ã€‚")
    else:
        st.info("è¯·ä¸Šä¼ ä¸€æ‰¹ç®€å†æ–‡ä»¶å¼€å§‹åˆ†æã€‚")

with tab3:
    st.subheader("å»é‡ & æ’åºï¼ˆå±•ç¤º Top-Nï¼‰")
    topn = st.slider("Top-N", 5, 50, 10)
    score_source = None
    if "score_df" in st.session_state:
        score_source = st.session_state["score_df"]
    elif "scored" in st.session_state:
        score_source = st.session_state["scored"]

    if score_source is not None:
        deduped = pipe.dedup_and_rank(score_source)
        st.session_state["shortlist"] = deduped.head(topn)
        # æ±‰åŒ–æ˜¾ç¤º
        deduped_display = translate_dataframe_columns(deduped.head(topn))
        st.dataframe(deduped_display, use_container_width=True)
    else:
        st.warning("è¯·å…ˆå®Œæˆè¯„åˆ†")

with tab4:
    st.subheader("ğŸ¤– ä¸€é”®é‚€çº¦ + è‡ªåŠ¨æ’æœŸ")
    st.markdown("è®©AIå¸®ä½ ç”Ÿæˆä¸ªæ€§åŒ–é‚€çº¦é‚®ä»¶ï¼ˆå«å€™é€‰äº®ç‚¹ + æ—¥å†é™„ä»¶ï¼‰")

    # ä¼˜å…ˆä½¿ç”¨å»é‡&æ’åºåçš„shortlistï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨åŸå§‹score_df
    shortlist = st.session_state.get("shortlist")
    score_df = st.session_state.get("score_df")
    
    if shortlist is not None and not shortlist.empty:
        # ä½¿ç”¨å»é‡&æ’åºåçš„ç»“æœ
        df = shortlist.copy()
        st.info(f"âœ… å·²ä½¿ç”¨ã€Œå»é‡&æ’åºã€æ­¥éª¤ç­›é€‰åçš„ Top-{len(df)} åå€™é€‰äºº")
    elif score_df is not None and not score_df.empty:
        # å¦‚æœæ²¡æœ‰shortlistï¼Œä½¿ç”¨åŸå§‹score_dfï¼ˆéœ€è¦å…ˆæ’åºï¼‰
        df = score_df.copy()
        # æŒ‰æ€»åˆ†é™åºæ’åº
        if "æ€»åˆ†" in df.columns:
            df = df.sort_values(by="æ€»åˆ†", ascending=False, ignore_index=True)
        elif "score_total" in df.columns:
            df = df.sort_values(by="score_total", ascending=False, ignore_index=True)
        st.warning("âš ï¸ å»ºè®®å…ˆåœ¨ã€Œå»é‡&æ’åºã€æ­¥éª¤ä¸­ç­›é€‰å€™é€‰äººï¼Œå½“å‰ä½¿ç”¨åŸå§‹è¯„åˆ†ç»“æœï¼ˆå·²æŒ‰æ€»åˆ†æ’åºï¼‰")
    else:
        st.warning("è¯·å…ˆå®ŒæˆAIåŒ¹é…è¯„åˆ†ã€‚")
        df = None
    
    if df is not None and not df.empty:
        max_candidates = len(df)
        default_top = min(5, max_candidates)
        top_n = st.number_input(
            "é€‰æ‹©è¦é‚€çº¦çš„å€™é€‰äººæ•°ï¼ˆTop-Nï¼‰",
            min_value=1,
            max_value=max_candidates,
            value=default_top,
            step=1,
        )
        top_n = int(top_n)
        selected_candidates = df.head(top_n)

        score_col = "æ€»åˆ†" if "æ€»åˆ†" in df.columns else "score_total" if "score_total" in df.columns else None
        display_cols = [
            col
            for col in [
                "name",
                "file",
                "email",
                "phone",
                score_col,
                "æŠ€èƒ½åŒ¹é…åº¦",
                "ç»éªŒç›¸å…³æ€§",
                "æˆé•¿æ½œåŠ›",
                "ç¨³å®šæ€§",
                "short_eval",
                "highlights",
                "resume_mini",
            ]
            if col and col in df.columns
        ]
        if not display_cols:
            display_cols = df.columns.tolist()

        st.write(f"å·²é€‰æ‹© {top_n} ä½å€™é€‰äººï¼š")
        st.dataframe(selected_candidates[display_cols], use_container_width=True)

        interview_time = st.text_input("ğŸ•’ é¢è¯•æ—¶é—´ï¼ˆä¾‹ï¼š2025-11-15 14:00, Asia/Shanghaiï¼‰", "2025-11-15 14:00, Asia/Shanghai")
        organizer_email = st.text_input("ğŸ“§ é¢è¯•ç»„ç»‡è€…é‚®ç®±", "hr@company.com")
        
        # ä¼ä¸šå¾®ä¿¡é…ç½®ï¼ˆå¯é€‰ï¼‰
        with st.expander("ğŸ“± ä¼ä¸šå¾®ä¿¡é…ç½®ï¼ˆå¯é€‰ï¼‰"):
            organizer_name = st.text_input("ç»„ç»‡è€…å§“å", "HR", help="ç”¨äºä¼ä¸šå¾®ä¿¡æ¶ˆæ¯ä¸­çš„è”ç³»äººæ˜¾ç¤º", key="organizer_name")
            organizer_wechat = st.text_input("ç»„ç»‡è€…ä¼ä¸šå¾®ä¿¡ID", "", help="å¯é€‰ï¼Œç”¨äºç”Ÿæˆä¼ä¸šå¾®ä¿¡æ·»åŠ é“¾æ¥", key="organizer_wechat")
            meeting_link = st.text_input("ä¼šè®®é“¾æ¥ï¼ˆå¯é€‰ï¼‰", "", help="å¦‚ï¼šè…¾è®¯ä¼šè®®é“¾æ¥ã€Zoomé“¾æ¥ç­‰", key="meeting_link")

        if st.button("ğŸš€ ä¸€é”®ç”Ÿæˆé‚€çº¦é‚®ä»¶ + ICS"):
            # è·å–ä¼ä¸šå¾®ä¿¡é…ç½®ï¼ˆå¦‚æœæœªè®¾ç½®ï¼Œä½¿ç”¨é»˜è®¤å€¼ï¼‰
            organizer_name = st.session_state.get("organizer_name", "HR")
            organizer_wechat = st.session_state.get("organizer_wechat", "")
            meeting_link = st.session_state.get("meeting_link", "")
            st.info("AI æ­£åœ¨ç”Ÿæˆä¸ªæ€§åŒ–é‚€çº¦å†…å®¹ï¼Œè¯·ç¨å€™...")

            invite_results = []
            invites_dir = "reports/invites"
            os.makedirs(invites_dir, exist_ok=True)

            job_title = st.session_state.get("job_name") or "ç›®æ ‡å²—ä½"

            for _, row in selected_candidates.iterrows():
                row_dict = row.to_dict()
                candidate_name = row_dict.get("file") or row_dict.get("name") or "åŒ¿åå€™é€‰äºº"
                candidate_email = row_dict.get("email", "")
                candidate_score = row_dict.get("æ€»åˆ†") or row_dict.get("score_total") or row_dict.get("score", "æœªçŸ¥")

                try:
                    candidate_highlight = generate_ai_summary(row_dict)
                except Exception as e:
                    candidate_highlight = f"AI æ€»ç»“å¤±è´¥ï¼š{e}"

                try:
                    ics_path = create_ics_file(
                        title=f"é¢è¯•é‚€çº¦ - {candidate_name}",
                        start_time=interview_time,
                        organizer=organizer_email,
                        attendee=candidate_email or "candidate@example.com",
                    )
                except Exception as e:
                    st.warning(f"ç”Ÿæˆ {candidate_name} çš„æ—¥å†æ–‡ä»¶å¤±è´¥ï¼š{e}")
                    ics_path = ""

                try:
                    email_body = generate_ai_email(
                        name=candidate_name,
                        highlights=candidate_highlight,
                        position=job_title,
                        score=candidate_score,
                        ics_path=ics_path or "(é™„ä»¶ç”Ÿæˆå¤±è´¥)",
                    )
                except Exception as e:
                    email_body = f"AI é‚®ä»¶ç”Ÿæˆå¤±è´¥ï¼š{e}"

                invite_results.append(
                    {
                        "name": candidate_name,
                        "email": candidate_email,
                        "ics": ics_path,
                        "body": email_body,
                        "highlights": candidate_highlight,
                        "score": candidate_score,
                        "position": job_title,
                        "interview_time": interview_time,
                    }
                )

            json_payload = json.dumps(invite_results, ensure_ascii=False, indent=2)
            json_path = os.path.join(invites_dir, f"invite_batch_{datetime.now().strftime('%Y%m%d_%H%M')}.json")
            with open(json_path, "w", encoding="utf-8") as fp:
                fp.write(json_payload)

            st.success("âœ… AI ä¸ªæ€§åŒ–é‚€çº¦ç”Ÿæˆå®Œæˆï¼")
            
            # ä¼ä¸šå¾®ä¿¡é›†æˆ
            st.markdown("### ğŸ“± ä¼ä¸šå¾®ä¿¡é‚€çº¦")
            try:
                from backend.services.wechat_integration import create_wechat_invite_template
                
                wechat_results = []
                for invite in invite_results:
                    wechat_data = create_wechat_invite_template({
                        "name": invite.get("name", ""),
                        "email": invite.get("email", ""),
                        "position": invite.get("position", job_title),
                        "interview_time": invite.get("interview_time", interview_time),
                        "highlights": invite.get("highlights", ""),
                        "meeting_link": meeting_link,
                        "organizer_name": organizer_name,
                        "organizer_wechat": organizer_wechat,
                    })
                    wechat_results.append(wechat_data)
                
                # æ˜¾ç¤ºä¼ä¸šå¾®ä¿¡æ¶ˆæ¯ï¼ˆå¯å¤åˆ¶ï¼‰
                for idx, (invite, wechat_data) in enumerate(zip(invite_results, wechat_results)):
                    with st.expander(f"ğŸ“± {invite.get('name', f'å€™é€‰äºº{idx+1}')} - ä¼ä¸šå¾®ä¿¡æ¶ˆæ¯"):
                        st.text_area(
                            "ä¼ä¸šå¾®ä¿¡æ¶ˆæ¯å†…å®¹ï¼ˆç‚¹å‡»å¤åˆ¶ï¼‰",
                            value=wechat_data.get("wechat_message", ""),
                            height=200,
                            key=f"wechat_msg_{idx}",
                            help="å¤åˆ¶æ­¤å†…å®¹åˆ°ä¼ä¸šå¾®ä¿¡å‘é€ç»™å€™é€‰äºº"
                        )
                        if wechat_data.get("meeting_link"):
                            st.write(f"ğŸ”— ä¼šè®®é“¾æ¥ï¼š{wechat_data.get('meeting_link')}")
                        if wechat_data.get("wechat_link"):
                            st.write(f"ğŸ“± {wechat_data.get('wechat_link')}")
            except Exception as e:
                st.info(f"ğŸ’¡ ä¼ä¸šå¾®ä¿¡åŠŸèƒ½ï¼š{str(e)}")
            
            # é‚®ä»¶å¯¼å…¥ä¼ä¸šé‚®ç®±
            st.markdown("### ğŸ“§ é‚®ä»¶å¯¼å…¥ä¼ä¸šé‚®ç®±")
            col1, col2 = st.columns(2)
            
            with col1:
                try:
                    from backend.services.email_integration import generate_email_import_file, generate_outlook_import_csv
                    
                    if st.button("ğŸ“¥ ç”Ÿæˆé‚®ä»¶å¯¼å…¥æ–‡ä»¶ï¼ˆ.emlï¼‰"):
                        with st.spinner("æ­£åœ¨ç”Ÿæˆé‚®ä»¶å¯¼å…¥æ–‡ä»¶..."):
                            import_path = generate_email_import_file(invite_results)
                            if import_path:
                                st.success(f"âœ… é‚®ä»¶æ–‡ä»¶å·²ç”Ÿæˆï¼š`{import_path}`")
                                st.info("ğŸ’¡ ä½¿ç”¨æ–¹æ³•ï¼š\n1. Outlookï¼šæ–‡ä»¶ -> æ‰“å¼€ -> å…¶ä»–æ–‡ä»¶ -> é€‰æ‹© .eml æ–‡ä»¶\n2. ä¼ä¸šé‚®ç®±ï¼šè®¾ç½® -> å¯¼å…¥é‚®ä»¶ -> é€‰æ‹© .eml æ–‡ä»¶")
                            else:
                                st.warning("âš ï¸ ç”Ÿæˆå¤±è´¥ï¼Œè¯·æ£€æŸ¥æ•°æ®")
                except Exception as e:
                    st.warning(f"é‚®ä»¶å¯¼å…¥åŠŸèƒ½ï¼š{str(e)}")
            
            with col2:
                try:
                    if st.button("ğŸ“‹ ç”ŸæˆOutlookå¯¼å…¥CSV"):
                        with st.spinner("æ­£åœ¨ç”ŸæˆCSVæ–‡ä»¶..."):
                            csv_path = generate_outlook_import_csv(invite_results)
                            if csv_path:
                                with open(csv_path, 'rb') as f:
                                    st.download_button(
                                        "â¬‡ï¸ ä¸‹è½½Outlookå¯¼å…¥CSV",
                                        data=f.read(),
                                        file_name=os.path.basename(csv_path),
                                        mime="text/csv"
                                    )
                                st.success(f"âœ… CSVæ–‡ä»¶å·²ç”Ÿæˆï¼š`{csv_path}`")
                except Exception as e:
                    st.warning(f"CSVç”ŸæˆåŠŸèƒ½ï¼š{str(e)}")
            
            # SMTPé‚®ä»¶å‘é€ï¼ˆå¯é€‰ï¼‰
            with st.expander("ğŸ“® é€šè¿‡SMTPç›´æ¥å‘é€é‚®ä»¶ï¼ˆéœ€è¦é…ç½®ï¼‰"):
                st.info("ğŸ’¡ éœ€è¦åœ¨ .env æ–‡ä»¶ä¸­é…ç½®ä»¥ä¸‹å‚æ•°ï¼š\n- SMTP_SERVERï¼ˆå¦‚ï¼šsmtp.exmail.qq.comï¼‰\n- SMTP_PORTï¼ˆé»˜è®¤587ï¼‰\n- SMTP_USERï¼ˆé‚®ç®±åœ°å€ï¼‰\n- SMTP_PASSWORDï¼ˆé‚®ç®±å¯†ç æˆ–æˆæƒç ï¼‰")
                
                smtp_server = st.text_input("SMTPæœåŠ¡å™¨", os.getenv("SMTP_SERVER", ""), help="å¦‚ï¼šsmtp.exmail.qq.com")
                smtp_port = st.number_input("SMTPç«¯å£", value=int(os.getenv("SMTP_PORT", "587")), min_value=1, max_value=65535)
                smtp_user = st.text_input("SMTPç”¨æˆ·åï¼ˆé‚®ç®±ï¼‰", os.getenv("SMTP_USER", ""))
                smtp_password = st.text_input("SMTPå¯†ç /æˆæƒç ", type="password", value=os.getenv("SMTP_PASSWORD", ""))
                
                if st.button("ğŸ“¤ æ‰¹é‡å‘é€é‚®ä»¶"):
                    if not smtp_server or not smtp_user or not smtp_password:
                        st.error("âŒ è¯·å…ˆé…ç½®SMTPå‚æ•°")
                    else:
                        try:
                            from backend.services.email_integration import send_email_via_smtp
                            
                            success_count = 0
                            fail_count = 0
                            
                            for invite in invite_results:
                                result = send_email_via_smtp(
                                    to_email=invite.get("email", ""),
                                    subject=f"é¢è¯•é‚€çº¦ - {job_title} - {invite.get('name', '')}",
                                    body=invite.get("body", ""),
                                    ics_path=invite.get("ics", ""),
                                    smtp_server=smtp_server,
                                    smtp_port=smtp_port,
                                    smtp_user=smtp_user,
                                    smtp_password=smtp_password,
                                    from_email=smtp_user
                                )
                                
                                if result.get("success"):
                                    success_count += 1
                                else:
                                    fail_count += 1
                                    st.warning(f"âŒ {invite.get('name', '')} å‘é€å¤±è´¥ï¼š{result.get('message', '')}")
                            
                            st.success(f"âœ… é‚®ä»¶å‘é€å®Œæˆï¼šæˆåŠŸ {success_count} å°ï¼Œå¤±è´¥ {fail_count} å°")
                        except Exception as e:
                            st.error(f"âŒ å‘é€å¤±è´¥ï¼š{str(e)}")
            
            st.download_button(
                "ğŸ“¥ ä¸‹è½½é‚€çº¦ç»“æœï¼ˆJSONï¼‰",
                data=json_payload,
                file_name="ai_invites.json",
                mime="application/json",
            )

            # ä¿å­˜å¾…é¢è¯•æ¸…å•ï¼ˆå¸¦é”™è¯¯å¤„ç†ï¼‰
            pending_path = "reports/pending_interviews.csv"
            try:
                # ç¡®ä¿ç›®å½•å­˜åœ¨
                import os
                os.makedirs("reports", exist_ok=True)
                
                # å°è¯•å†™å…¥æ–‡ä»¶
                pd.DataFrame(invite_results).to_csv(pending_path, index=False, encoding="utf-8-sig")
                st.write(f"ğŸ“‹ å·²è‡ªåŠ¨æ›´æ–°å¾…é¢è¯•æ¸…å•ï¼š`{pending_path}`")
            except PermissionError:
                # å¦‚æœæ–‡ä»¶è¢«å ç”¨ï¼ˆå¦‚ Excel æ‰“å¼€ï¼‰ï¼Œä½¿ç”¨å¸¦æ—¶é—´æˆ³çš„æ–‡ä»¶å
                from datetime import datetime
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                pending_path_alt = f"reports/pending_interviews_{timestamp}.csv"
                try:
                    pd.DataFrame(invite_results).to_csv(pending_path_alt, index=False, encoding="utf-8-sig")
                    st.warning(f"âš ï¸ åŸæ–‡ä»¶è¢«å ç”¨ï¼Œå·²ä¿å­˜åˆ°ï¼š`{pending_path_alt}`")
                    st.info("ğŸ’¡ æç¤ºï¼šè¯·å…³é—­å¯èƒ½æ­£åœ¨æ‰“å¼€ `pending_interviews.csv` çš„ç¨‹åºï¼ˆå¦‚ Excelï¼‰")
                except Exception as e:
                    st.warning(f"âš ï¸ ä¿å­˜å¾…é¢è¯•æ¸…å•å¤±è´¥ï¼š{str(e)}")
            except Exception as e:
                st.warning(f"âš ï¸ ä¿å­˜å¾…é¢è¯•æ¸…å•å¤±è´¥ï¼š{str(e)}")

            st.json(invite_results, expanded=False)

with tab5:
    st.subheader("é¢è¯•åŒ… & å¯¼å‡ºæŠ¥è¡¨")
    if st.button("å¯¼å‡ºæœ¬è½®æŠ¥è¡¨"):
        score_df = st.session_state.get("score_df", None)
        scored_df = st.session_state.get("scored", None)

        if score_df is not None and not score_df.empty:
            score_source = score_df
        elif scored_df is not None and not scored_df.empty:
            score_source = scored_df
        else:
            st.warning("æœªæ‰¾åˆ°å¯å¯¼å‡ºçš„è¯„åˆ†æ•°æ®ï¼Œè¯·å…ˆå®Œæˆ AI åŒ¹é…è¯„åˆ†ã€‚")
            st.stop()

        path = export_round_report(score_source)
        st.success("å·²å¯¼å‡ºï¼š" + path)

