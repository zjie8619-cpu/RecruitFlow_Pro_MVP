"""
Lightweight heuristic matcher used when the heavy AI pipeline is unavailable.
It scores resumes against a JD text and returns a dataframe with scoring columns
expected by the Streamlit UI.
"""

from __future__ import annotations

import json
import math
import re
from typing import Any, Dict, Iterable, List, Sequence, Tuple

import pandas as pd

from backend.services.text_rules import sanitize_for_job, strip_competition_terms


SPLIT_PATTERN = re.compile(r"[ã€‚.!?ï¼›;ï¼Œ,\n]+")
WORD_PATTERN = re.compile(r"[A-Za-z0-9\u4e00-\u9fa5]{2,}")

GROWTH_KEYWORDS = ("å­¦ä¹ ", "æˆé•¿", "å¤ç›˜", "æ”¹è¿›", "è‡ªæˆ‘é©±åŠ¨", "è¿­ä»£")
STABILITY_KEYWORDS = ("ç¨³å®š", "é•¿æœŸ", "è¿ç»­", "ä»»èŒ", "ç•™ä»»", "å¹´åº¦")

EDUCATION_BOOST_RULES = [
    (("åšå£«", "PhD", "Doctor", "åšå£«å"), 12, 10),
    (("ç¡•å£«", "Master", "ç ”ç©¶ç”Ÿ"), 8, 6),
    (("æœ¬ç§‘", "å­¦å£«", "Bachelor"), 4, 3),
]

DIME_META = {
    "æŠ€èƒ½åŒ¹é…åº¦": {"max": 30.0, "weight": 0.30},
    "ç»éªŒç›¸å…³æ€§": {"max": 30.0, "weight": 0.30},
    "æˆé•¿æ½œåŠ›": {"max": 20.0, "weight": 0.20},
    "ç¨³å®šæ€§": {"max": 20.0, "weight": 0.20},
}

DOMAIN_KEYWORDS = {
    "ç‰©ç†", "ç«èµ›", "æ•™ç»ƒ", "å¥¥èµ›", "imo", "cupt", "æ•™å­¦", "æ•™ç ”", "å®éªŒ",
    "ç§‘ç ”", "è¯¾ç¨‹", "è¯¾å ‚", "æ•™æ", "latex", "æ•™æ¡ˆ", "è¾…å¯¼", "å­¦ç”Ÿ", "èµ›é¢˜"
}


def _tokenize(text: str) -> List[str]:
    if not text:
        return []
    return [tok.lower() for tok in WORD_PATTERN.findall(text)]


def _top_keywords(tokens: Sequence[str], limit: int = 25) -> set[str]:
    freq: dict[str, int] = {}
    for tok in tokens:
        freq[tok] = freq.get(tok, 0) + 1
    sorted_tokens = sorted(freq.items(), key=lambda item: item[1], reverse=True)
    return {tok for tok, _ in sorted_tokens[:limit]}


def _length_score(text_len: int) -> float:
    if text_len >= 4500:
        return 95
    if text_len >= 3000:
        return 88
    if text_len >= 2000:
        return 80
    if text_len >= 1200:
        return 68
    if text_len >= 600:
        return 55
    return 45


def _keyword_overlap_score(resume_tokens: set[str], job_tokens: set[str]) -> float:
    if not job_tokens:
        return 75
    overlap = len(resume_tokens & job_tokens)
    ratio = overlap / max(len(job_tokens), 1)
    # scale ratio (0-1) into 45-98
    return max(45.0, min(98.0, 45.0 + ratio * 70.0))


def _count_keywords(text: str, keywords: Iterable[str]) -> int:
    lowered = text.lower()
    return sum(lowered.count(word.lower()) for word in keywords)


def _growth_score(text: str) -> float:
    hits = _count_keywords(text, GROWTH_KEYWORDS)
    if hits >= 6:
        return 92
    if hits >= 4:
        return 82
    if hits >= 2:
        return 73
    if hits >= 1:
        return 64
    return 55


def _stability_score(text: str) -> float:
    hits = _count_keywords(text, STABILITY_KEYWORDS)
    if hits >= 5:
        return 90
    if hits >= 3:
        return 78
    if hits >= 1:
        return 68
    return 60


def _collect_evidence(resume_text: str, job_tokens: set[str]) -> str:
    if not resume_text:
        return ""
    segments = [seg.strip() for seg in SPLIT_PATTERN.split(resume_text) if seg.strip()]
    if not segments:
        return ""

    scored: List[Tuple[int, str]] = []
    lowered_tokens = {tok.lower() for tok in job_tokens}
    for seg in segments:
        seg_tokens = _tokenize(seg)
        overlap = len(lowered_tokens & set(seg_tokens))
        if overlap:
            scored.append((overlap, seg))

    scored.sort(key=lambda item: item[0], reverse=True)
    evidence_segments = [seg for _, seg in scored[:3]] or segments[:2]
    return "ï¼›".join(evidence_segments)


def _education_boost(text: str) -> tuple[float, float]:
    """Return (skill_boost, growth_boost) based on education level mentioned."""
    lowered = text.lower()
    for keywords, skill_boost, growth_boost in EDUCATION_BOOST_RULES:
        if any(keyword.lower() in lowered for keyword in keywords):
            return float(skill_boost), float(growth_boost)
    return 0.0, 0.0


def _domain_boost(resume_tokens: set[str], jd_tokens: set[str]) -> float:
    """Give extra skill points when resume explicitlyæåˆ° JD å…³é”®æœ¯è¯­ã€‚"""
    keywords = DOMAIN_KEYWORDS | {tok for tok in jd_tokens if tok in DOMAIN_KEYWORDS}
    hits = len(resume_tokens & keywords)
    if hits == 0:
        return 0.0
    return min(15.0, 4.0 * hits)


def _normalize_score(value: float, minimum: float = 0.0, maximum: float = 100.0) -> float:
    return max(minimum, min(maximum, value))


def _parse_ai_json(raw_content: str) -> dict:
    """
    è§£æå¤§æ¨¡å‹è¿”å›çš„ JSON å†…å®¹ï¼Œå®¹é”™å¤„ç† Markdown æˆ–é¢å¤–æ–‡æœ¬ã€‚
    """
    if not raw_content:
        raise ValueError("empty content")

    candidates: List[str] = [raw_content.strip()]

    # å»é™¤å¸¸è§çš„ä»£ç å—åŒ…è£…
    if candidates[0].startswith("```"):
        stripped = re.sub(r"^```[a-zA-Z0-9_-]*", "", candidates[0]).strip()
        stripped = re.sub(r"```$", "", stripped).strip()
        candidates.append(stripped)

    # æˆªå–ç¬¬ä¸€ä¸ªå¤§æ‹¬å·åˆ°æœ€åä¸€ä¸ªå¤§æ‹¬å·ä¹‹é—´çš„å†…å®¹
    start = raw_content.find("{")
    end = raw_content.rfind("}")
    if 0 <= start < end:
        candidates.append(raw_content[start : end + 1])

    for cand in candidates:
        try:
            return json.loads(cand)
        except Exception:
            continue
    raise ValueError("unable to parse AI JSON response")


def _short_eval(total: float, skill: float, exp: float, growth: float) -> str:
    return (
        f"æ€»ä½“ {total:.0f} åˆ†ï½œæŠ€èƒ½ {skill:.0f}ï½œç»éªŒ {exp:.0f}ï½œ"
        f"æˆé•¿ {growth:.0f}"
    )


def _heuristic_score_from_text(
    jd_text: str, resume_text: str, job_title: str = ""
) -> Dict[str, Any]:
    """
    å½“å¤§æ¨¡å‹æ‰“åˆ†å¤±è´¥æ—¶ï¼Œä½¿ç”¨æœ¬åœ°å¯å‘å¼è§„åˆ™ç»™å‡ºä¸€ä¸ªâ€œè¿˜ç®—åˆç†â€çš„è¯„åˆ†ï¼Œé¿å…å‡ºç°å…¨ 0 åˆ†ã€‚
    è¯„åˆ†ç»´åº¦ä¸å‰ç«¯å±•ç¤ºä¿æŒä¸€è‡´ï¼šæ€»åˆ† / æŠ€èƒ½åŒ¹é…åº¦ / ç»éªŒç›¸å…³æ€§ / æˆé•¿æ½œåŠ› / ç¨³å®šæ€§ã€‚
    """
    jd_clean = strip_competition_terms(jd_text or "", job_title or "")
    job_tokens = _top_keywords(_tokenize(jd_clean))

    resume_tokens = set(_tokenize(resume_text or ""))
    text_len = len(resume_text or "")

    skill_score = _keyword_overlap_score(resume_tokens, job_tokens)
    exp_score = _length_score(text_len)
    growth_score = _growth_score(resume_text or "")
    stability_score = _stability_score(resume_text or "")

    # åŠ æƒï¼šæ•™è‚²ç¨‹åº¦ã€ä¸“ä¸šå…³é”®è¯
    edu_skill_boost, edu_growth_boost = _education_boost(resume_text or "")
    domain_boost = _domain_boost(resume_tokens, job_tokens)
    skill_score = _normalize_score(skill_score + edu_skill_boost + domain_boost, 0, 100)
    growth_score = _normalize_score(growth_score + edu_growth_boost, 0, 100)

    # ç»éªŒåˆ†é¢å¤–è€ƒè™‘æ•™è‚²èƒŒæ™¯ï¼ˆç¡•åšç»å†é€šå¸¸ä¼´éšç§‘ç ”ç»éªŒï¼‰
    exp_score = _normalize_score(exp_score + edu_skill_boost * 0.4, 0, 100)

    total = (
        skill_score * 0.45
        + exp_score * 0.25
        + growth_score * 0.2
        + stability_score * 0.1
    )
    total = round(total, 1)

    evidence = _collect_evidence(resume_text or "", job_tokens)
    short_eval = f"[å¯å‘å¼] {_short_eval(total, skill_score, exp_score, growth_score)}"

    return {
        "æ€»åˆ†": total,
        "ç»´åº¦å¾—åˆ†": {
            "æŠ€èƒ½åŒ¹é…åº¦": round(skill_score, 1),
            "ç»éªŒç›¸å…³æ€§": round(exp_score, 1),
            "æˆé•¿æ½œåŠ›": round(growth_score, 1),
            "ç¨³å®šæ€§": round(stability_score, 1),
        },
        "è¯æ®": [evidence] if evidence else [],
        "ç®€è¯„": short_eval,
        "short_eval": short_eval,
    }


def _normalize_ai_scores(data: Dict[str, Any]) -> Tuple[Dict[str, Any], bool]:
    """
    å°†å¤§æ¨¡å‹è¿”å›çš„ 0-30 / 0-20 åˆ†åˆ¶ï¼Œç»Ÿä¸€æ˜ å°„åˆ° 0-100ã€‚
    è¿”å› (è§„èŒƒåŒ–åçš„ data, æ˜¯å¦æ‰€æœ‰ç»´åº¦éƒ½æ˜¯ 0)ã€‚
    """
    if not isinstance(data, dict):
        raise ValueError("AI è¿”å›å€¼ä¸æ˜¯ JSON å¯¹è±¡")

    raw_dims = data.get("ç»´åº¦å¾—åˆ†")
    if not isinstance(raw_dims, dict):
        raise ValueError("AI è¿”å›å€¼ç¼ºå°‘ `ç»´åº¦å¾—åˆ†` å­—æ®µ")

    normalized_dims: Dict[str, float] = {}
    all_zero = True

    total_score = 0.0
    for key, meta in DIME_META.items():
        if key not in raw_dims:
            raise ValueError(f"AI è¿”å›å€¼ç¼ºå°‘ `{key}` åˆ†æ•°")
        raw_value = raw_dims[key]
        try:
            value = float(raw_value)
        except (TypeError, ValueError):
            raise ValueError(f"{key} åˆ†æ•°ä¸æ˜¯æ•°å­—ï¼š{raw_value!r}")

        if value > 0:
            all_zero = False

        max_value = meta["max"]
        normalized_value = _normalize_score(value / max_value * 100.0, 0.0, 100.0)
        normalized_dims[key] = round(normalized_value, 1)
        total_score += normalized_value * meta["weight"]

    data["ç»´åº¦å¾—åˆ†"] = normalized_dims
    data["æ€»åˆ†"] = round(_normalize_score(total_score, 0.0, 100.0), 1)
    return data, all_zero


def _heuristic_match_resumes_df(
    jd_text: str,
    resumes_df: pd.DataFrame,
    job_title: str | None = None,
) -> pd.DataFrame:
    """
    çº¯å¯å‘å¼çš„åŒ¹é…ç®—æ³•ï¼Œä¾› AI æ‰“åˆ†å¤±è´¥æ—¶ fallback ä½¿ç”¨ï¼Œä¹Ÿå¯åœ¨è°ƒè¯•é˜¶æ®µå•ç‹¬è°ƒç”¨ã€‚
    """

    if resumes_df is None or resumes_df.empty:
        raise ValueError("resumes_df ä¸ºç©ºï¼Œæ— æ³•åŒ¹é…")

    jd_clean = strip_competition_terms(jd_text or "", job_title or "")
    job_tokens = _top_keywords(_tokenize(jd_clean))

    scored_rows = []
    for _, row in resumes_df.iterrows():
        resume_text = str(row.get("resume_text", "") or "")
        tokens = set(_tokenize(resume_text))
        text_len = int(row.get("text_len") or len(resume_text))

        skill_score = _keyword_overlap_score(tokens, job_tokens)
        exp_score = _length_score(text_len)
        growth_score = _growth_score(resume_text)
        stability_score = _stability_score(resume_text)

        total = (
            skill_score * 0.4
            + exp_score * 0.3
            + growth_score * 0.2
            + stability_score * 0.1
        )
        total = round(total, 1)

        evidence = _collect_evidence(resume_text, job_tokens)
        if evidence:
            evidence, _ = sanitize_for_job(job_title or "", evidence, evidence)

        short_eval = _short_eval(total, skill_score, exp_score, growth_score)

        enriched = row.to_dict()
        enriched.update(
            {
                "æŠ€èƒ½åŒ¹é…åº¦": round(skill_score, 1),
                "ç»éªŒç›¸å…³æ€§": round(exp_score, 1),
                "æˆé•¿æ½œåŠ›": round(growth_score, 1),
                "ç¨³å®šæ€§": round(stability_score, 1),
                "æ€»åˆ†": total,
                "short_eval": short_eval,
                "è¯æ®": evidence,
            }
        )
        scored_rows.append(enriched)

    result = pd.DataFrame(scored_rows)
    result.sort_values(by="æ€»åˆ†", ascending=False, inplace=True, ignore_index=True)
    return result

# åœ¨å¯¼å…¥å…¶ä»–æ¨¡å—ä¹‹å‰ï¼Œå…ˆè®¾ç½® stdout ç¼–ç ä¿æŠ¤
try:
    if sys.stdout.encoding and sys.stdout.encoding.lower() not in ('utf-8', 'utf8'):
        # åŒ…è£… stdout ä»¥å¤„ç†ç¼–ç é”™è¯¯
        if not hasattr(sys.stdout, '_original_write'):
            _original_stdout_write = sys.stdout.write
            def _safe_stdout_write(s):
                try:
                    _original_stdout_write(s)
                except (UnicodeEncodeError, UnicodeError):
                    # å°è¯•ç”¨ UTF-8 ç¼–ç å¹¶æ›¿æ¢æ— æ³•ç¼–ç çš„å­—ç¬¦
                    try:
                        safe_s = s.encode('utf-8', errors='replace').decode('utf-8', errors='replace')
                        _original_stdout_write(safe_s)
                    except Exception:
                        pass  # å¦‚æœè¿˜æ˜¯å¤±è´¥ï¼Œå°±å¿½ç•¥
            sys.stdout.write = _safe_stdout_write
            sys.stdout._original_write = _original_stdout_write
except Exception:
    pass  # å¦‚æœè®¾ç½®å¤±è´¥ï¼Œç»§ç»­æ‰§è¡Œ

from backend.services.ai_client import get_client_and_cfg, chat_completion
from backend.services.competency_utils import determine_competency_strategy
from backend.utils.sanitize import sanitize_ai_output, SYSTEM_PROMPT
from backend.services.text_rules import sanitize_for_job, infer_job_family


def _safe_str(obj):
    """å®‰å…¨åœ°å°†å¯¹è±¡è½¬æ¢ä¸ºå­—ç¬¦ä¸²ï¼Œå¤„ç†ç¼–ç é”™è¯¯"""
    if obj is None:
        return ""
    try:
        # å¦‚æœå·²ç»æ˜¯å­—ç¬¦ä¸²ï¼Œç›´æ¥è¿”å›
        if isinstance(obj, str):
            return obj
        # å°è¯•æ­£å¸¸è½¬æ¢
        return str(obj)
    except (UnicodeEncodeError, UnicodeError):
        # å¦‚æœè½¬æ¢å¤±è´¥ï¼Œä½¿ç”¨å®‰å…¨çš„ç¼–ç æ–¹å¼
        try:
            if isinstance(obj, str):
                return obj.encode('utf-8', errors='replace').decode('utf-8', errors='replace')
            else:
                # å…ˆè½¬æ¢ä¸ºå­—ç¬¦ä¸²ï¼Œå†ç¼–ç 
                s = str(obj)
                return s.encode('utf-8', errors='replace').decode('utf-8', errors='replace')
        except Exception:
            # å¦‚æœè¿˜æ˜¯å¤±è´¥ï¼Œè¿”å›ç©ºå­—ç¬¦ä¸²
            return ""


def _safe_join(items, separator="ï¼›"):
    """å®‰å…¨åœ°è¿æ¥å­—ç¬¦ä¸²åˆ—è¡¨ï¼Œå¤„ç†ç¼–ç é”™è¯¯"""
    try:
        return separator.join(_safe_str(item) for item in items if item)
    except (UnicodeEncodeError, UnicodeError):
        # å¦‚æœè¿æ¥æ—¶å‡ºé”™ï¼Œå°è¯•é€ä¸ªå®‰å…¨è½¬æ¢
        safe_items = []
        for item in items:
            if item:
                try:
                    safe_items.append(_safe_str(item))
                except Exception:
                    continue
        return separator.join(safe_items) if safe_items else ""


def _safe_print(*args, **kwargs):
    """å®‰å…¨çš„ print å‡½æ•°ï¼Œå¤„ç† Windows GBK ç¼–ç é”™è¯¯"""
    try:
        print(*args, **kwargs)
    except UnicodeEncodeError:
        # å¦‚æœé‡åˆ°ç¼–ç é”™è¯¯ï¼Œä½¿ç”¨ errors='replace' æˆ– 'ignore' å¤„ç†
        try:
            # å°è¯•å°†è¾“å‡ºç¼–ç ä¸º UTF-8
            if sys.stdout.encoding and sys.stdout.encoding.lower() != 'utf-8':
                # ä¸´æ—¶è®¾ç½® stdout ç¼–ç 
                old_stdout = sys.stdout
                sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')
                try:
                    print(*args, **kwargs)
                finally:
                    sys.stdout = old_stdout
            else:
                # ç›´æ¥ä½¿ç”¨ replace æ¨¡å¼
                safe_args = []
                for arg in args:
                    if isinstance(arg, str):
                        safe_args.append(arg.encode('utf-8', errors='replace').decode('utf-8', errors='replace'))
                    else:
                        safe_args.append(str(arg).encode('utf-8', errors='replace').decode('utf-8', errors='replace'))
                print(*safe_args, **kwargs)
        except Exception:
            # å¦‚æœè¿˜æ˜¯å¤±è´¥ï¼Œå°±å¿½ç•¥è¿™ä¸ª print
            pass


def _get_model(cfg: Any) -> str:
    if hasattr(cfg, "model"):
        return cfg.model
    if isinstance(cfg, dict):
        return cfg.get("model", "gpt-4o-mini")
    return "gpt-4o-mini"


def _get_temperature(cfg: Any) -> float:
    if hasattr(cfg, "temperature"):
        return float(getattr(cfg, "temperature"))
    if isinstance(cfg, dict):
        return float(cfg.get("temperature", 0.6))
    return 0.6


SHORT_EVAL_PROMPT = """
ä½ æ˜¯ä¸€åä¸“ä¸šçš„æ•™è‚²è¡Œä¸š HRï¼Œè¯·åŸºäºå€™é€‰äººçš„çœŸå®ç®€å†å†…å®¹ï¼Œç”¨ä¸€å¥ä¸­æ–‡ç”Ÿæˆ 20~40 å­—çš„é«˜åº¦æ¦‚æ‹¬è¯„ä»·ã€‚

è¦æ±‚ï¼š
- å¿…é¡»ä»ç®€å†å†…å®¹ä¸­æç‚¼ï¼Œç¦æ­¢ä½¿ç”¨æ¨¡æ¿å¥
- å¿…é¡»å‡†ç¡®åæ˜ å€™é€‰äººçš„ä¸“ä¸šèƒŒæ™¯ã€ç»éªŒç‰¹ç‚¹æˆ–äº®ç‚¹
- å¦‚æœæ˜¯æ•™å¸ˆ/æ•™ç»ƒå²—ä½ï¼Œä¸¥ç¦å‡ºç°"é”€å”®ã€å¼€å‘å®¢æˆ·ã€æ‹‰æ–°ã€è½¬åŒ–ã€é‚€çº¦ã€ç”µé”€"ç­‰ä¸æ•™è‚²æ— å…³çš„è¯
- å…è®¸ä½¿ç”¨"æ²Ÿé€šã€è´Ÿè´£ã€æˆè¯¾ã€å®¶é•¿ã€å­¦ç”Ÿã€æ•™å­¦"ç­‰æ•™è‚²è¡Œä¸šæ­£å¸¸è¯æ±‡
- ä¸å¾—æé€ ä¸å­˜åœ¨çš„ç»å†
- ä¸å¾—è¾“å‡º"ç®€å†ä¿¡æ¯ä¸è¶³"æˆ–ç±»ä¼¼è¯æœ¯
- è‹¥æ–‡æœ¬ä¸ºç©ºï¼Œåˆ™ç›´æ¥è¿”å›ï¼š"ç®€å†è§£æå¤±è´¥ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶æ ¼å¼"

ã€ç®€å†å†…å®¹ã€‘
{resume_text}
"""


def _prepare_resume_text(file_text: str) -> str:
    """
    æ–°é€»è¾‘ï¼šç¡®ä¿å®Œæ•´ç®€å†ä¸è¢« LLM æˆªæ–­ã€‚
    å°†å…¨æ–‡å¼ºåˆ¶åˆ†æˆ 2500~3000 å­—çš„ç‰‡æ®µï¼Œæ¨¡å‹ä¼šæŒ‰é¡ºåºé˜…è¯»ã€‚
    """
    text = file_text.strip()
    if not text:
        return text
    
    size = 2800
    chunks = []
    for i in range(0, len(text), size):
        part = text[i:i+size]
        chunks.append(f"ã€Resume Part {len(chunks)+1}ã€‘\n{part}")
    
    return "\n\n".join(chunks)


def _generate_short_eval(client, cfg, resume_text: str, job_title: str) -> str:
    """
    ç”Ÿæˆå€™é€‰äººçš„ç®€çŸ­è¯„ä»·ï¼ˆshort_evalï¼‰
    ç¡®ä¿è¿”å›çœŸå®çš„ AI è¯„ä»·ï¼Œè€Œä¸æ˜¯å¼‚å¸¸æç¤º
    """
    cleaned_text = (resume_text or "").strip()
    if not cleaned_text:
        return "ç®€å†è§£æå¤±è´¥ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶æ ¼å¼"

    try:
        # ä½¿ç”¨åˆ†æ®µé€»è¾‘ï¼Œç¡®ä¿å®Œæ•´ä¼ å…¥ï¼ˆç®€è¯„ä¹Ÿéœ€è¦çœ‹åˆ°å®Œæ•´ç®€å†ï¼‰
        prepared_resume = _prepare_resume_text(cleaned_text)
        prompt = SHORT_EVAL_PROMPT.format(resume_text=prepared_resume)
        
        res = chat_completion(
            client,
            cfg,
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€åä¸“ä¸šçš„æ•™è‚²è¡Œä¸š HRï¼Œæ“…é•¿ä»ç®€å†ä¸­æç‚¼å€™é€‰äººäº®ç‚¹ã€‚"},
                {"role": "user", "content": prompt}
            ],
            temperature=0.4,
            max_tokens=150,
        )
        content = res["choices"][0]["message"]["content"].strip()
        
        # è½»åº¦æ¸…æ´—ï¼šåªå»é™¤æ˜æ˜¾çš„é”€å”®è¯æ±‡ï¼Œä¿ç•™åŸå§‹è¯„ä»·
        if content:
            # å¯¹äºæ•™è‚²è¡Œä¸šå²—ä½ï¼Œåªå»é™¤æ˜æ˜¾ä¸ç›¸å…³çš„è¯æ±‡
            education_keywords = ["è¯¾ç¨‹", "é¡¾é—®", "æ•™å¸ˆ", "æ•™ç»ƒ", "æ‹›ç”Ÿ", "å­¦ç®¡"]
            is_education = any(k in job_title for k in education_keywords)
            
            if is_education:
                # æ•™è‚²è¡Œä¸šï¼šåªå»é™¤é”€å”®è¯æ±‡ï¼Œä¿ç•™å…¶ä»–æ‰€æœ‰å†…å®¹
                sales_words = ["å¼€å‘å®¢æˆ·", "æ‹‰æ–°", "è½¬åŒ–", "é‚€çº¦", "ç”µé”€"]
                for word in sales_words:
                    content = content.replace(word, "")
                content = re.sub(r"\s+", " ", content).strip()
            else:
                # éæ•™è‚²è¡Œä¸šï¼šè½»åº¦æ¸…æ´—ï¼Œä½†ä¿ç•™åŸå§‹å†…å®¹
                content = sanitize_ai_output(content, job_title)
                # å¦‚æœè¢«æ›¿æ¢ä¸ºå¼‚å¸¸æç¤ºï¼Œå°è¯•ä½¿ç”¨åŸå§‹å†…å®¹
                if "å­˜åœ¨å¼‚å¸¸" in content:
                    # å›é€€åˆ°åŸå§‹å†…å®¹ï¼Œåªåšæœ€åŸºæœ¬çš„æ¸…ç†
                    content = res["choices"][0]["message"]["content"].strip()
        
        # ç¡®ä¿ short_eval æ°¸ä¸è¢«æ¸…ç©ºæˆ–è¢«æ›¿æ¢ä¸ºå¼‚å¸¸æç¤º
        if not content or not content.strip() or "å­˜åœ¨å¼‚å¸¸" in content:
            # å¦‚æœå†…å®¹ä¸ºç©ºæˆ–è¢«æ›¿æ¢ä¸ºå¼‚å¸¸ï¼Œä½¿ç”¨åŸå§‹ AI è¿”å›
            original_content = res["choices"][0]["message"]["content"].strip()
            if original_content and len(original_content) > 10:
                content = original_content[:100]  # ä½¿ç”¨åŸå§‹å†…å®¹çš„å‰100å­—ç¬¦
            else:
                # æœ€åçš„å…œåº•ï¼šç”Ÿæˆä¸€ä¸ªé€šç”¨çš„è¯„ä»·
                content = "è¯¥å€™é€‰äººå…·å¤‡ç›¸å…³å·¥ä½œç»éªŒï¼Œè¯·ç»“åˆç®€å†è¿›ä¸€æ­¥è¯„ä¼°ã€‚"
        
        return content
    except Exception as err:
        # API è°ƒç”¨å¤±è´¥æ—¶ï¼Œè¿”å›é”™è¯¯ä¿¡æ¯è€Œä¸æ˜¯å¼‚å¸¸æç¤º
        error_msg = f"AIè¯„ä»·ç”Ÿæˆå¤±è´¥ï¼š{str(err)[:50]}"
        return error_msg


def ai_score_one(client, cfg, jd_text: str, resume_text: str, job_title: str = "") -> Dict[str, Any]:
    """
    å¯¹å•ä¸ªå€™é€‰äººè¿›è¡Œ AI è¯„åˆ†
    æ‰€æœ‰å­—ç¬¦ä¸²å¤„ç†éƒ½ä½¿ç”¨å®‰å…¨çš„ç¼–ç æ–¹å¼
    """
    try:
        # ä½¿ç”¨ç»Ÿä¸€çš„é˜²å¹»è§‰ç³»ç»Ÿæç¤ºè¯
        # æ­¥éª¤1ï¼šå¼ºåˆ¶åˆ†æ®µï¼Œç¡®ä¿å®Œæ•´ä¼ å…¥
        prepared_resume = _prepare_resume_text(resume_text)

        prompt = f"""
ä½ æ˜¯èµ„æ·±æ‹›è˜é¢è¯•å®˜ã€‚è¯·åŸºäºä¸‹é¢ä¿¡æ¯å¯¹å€™é€‰äººè¿›è¡ŒåŒ¹é…è¯„åˆ†ï¼Œè¿”å›ä¸­æ–‡ JSONï¼Œä¸”åªè¿”å› JSONï¼š

ã€å²—ä½ JDã€‘
{jd_text}

ã€å€™é€‰äººç®€å†ã€‘
{prepared_resume}

è¯„åˆ†å£å¾„ï¼ˆæ€»åˆ† 100ï¼‰ï¼š
- æŠ€èƒ½åŒ¹é…åº¦ï¼ˆ30ï¼‰
- ç»éªŒç›¸å…³æ€§ï¼ˆ30ï¼‰
- æˆé•¿æ½œåŠ›ï¼ˆ20ï¼‰
- ç¨³å®šæ€§ä¸å²—ä½é€‚é…æ€§ï¼ˆ20ï¼‰

è¯·æ ¹æ®ä½ èƒ½è¯†åˆ«åˆ°çš„ä¿¡æ¯è¿›è¡Œè¯„åˆ†ã€‚
æŸäº›å­—æ®µç¼ºå¤±ï¼ˆå¦‚é¡¹ç›®/æ•™è‚²/æŠ€èƒ½ï¼‰å±äºæ­£å¸¸æƒ…å†µï¼Œä¸è¦è¿”å›"ä¿¡æ¯ä¸è¶³"ã€‚
å¦‚æœæŸéƒ¨åˆ†ç¼ºå¤±ï¼Œè¯·åœ¨è¾“å‡ºä¸­æ³¨æ˜ï¼š
"æ­¤éƒ¨åˆ†ä¿¡æ¯ç¼ºå¤±ï¼Œå·²æŒ‰å·²æœ‰ä¿¡æ¯è¿›è¡Œä¼°ç®—ã€‚"

æ°¸è¿œä¸è¦è¿”å›"ä¿¡æ¯ä¸è¶³"ã€‚

è¾“å‡ºä¸¥æ ¼ JSONï¼š
{{
  "æ€»åˆ†": <0-100çš„æ•´æ•°>,
  "ç»´åº¦å¾—åˆ†": {{
    "æŠ€èƒ½åŒ¹é…åº¦": <0-30>,
    "ç»éªŒç›¸å…³æ€§": <0-30>,
    "æˆé•¿æ½œåŠ›": <0-20>,
    "ç¨³å®šæ€§": <0-20>
  }},
  "è¯æ®": ["ä½¿ç”¨ç®€å†ä¸­çš„å¼•ç”¨è¯­å¥æˆ–è¦ç‚¹ï¼Œ2-4æ¡"],
  "ç®€è¯„": "ä¸€å¥ä¸­æ–‡æ€»ç»“"
}}
åªè¿”å› JSON å¯¹è±¡ï¼Œä¸èƒ½åŒ…å«ä»»ä½•è§£é‡Šã€‚
"""
        res = chat_completion(
            client,
            cfg,
            messages=[
                {"role": "system", "content": SYSTEM_PROMPT},
                {"role": "user", "content": prompt}
            ],
            temperature=_get_temperature(cfg),
        )
        
        # æ­¥éª¤3ï¼šJSON è¾“å‡ºå®¹é”™è¡¥ä¸
        raw_content = res["choices"][0]["message"]["content"]
        try:
            data = _parse_ai_json(raw_content)
        except Exception as e:
            fallback = _heuristic_score_from_text(jd_text, resume_text, job_title)
            fallback["è§£æé”™è¯¯"] = _safe_str(e)[:100]
            return fallback

        try:
            data, dimensions_all_zero = _normalize_ai_scores(data)
        except Exception as e:
            fallback = _heuristic_score_from_text(jd_text, resume_text, job_title)
            fallback["è§£æé”™è¯¯"] = _safe_str(e)[:100]
            return fallback
        
        # ğŸš« é˜²å¹»è§‰è¿‡æ»¤ï¼šæ¸…ç†"è¯æ®"å’Œ"ç®€è¯„"ï¼ˆä¼˜åŒ–ç‰ˆï¼Œé¿å…è¿‡åº¦æ¸…æ´—ï¼‰
        if job_title:
            evidence_list = data.get("è¯æ®", [])
        
        # åˆ¤æ–­æ˜¯å¦ä¸ºæ•™è‚²è¡Œä¸šå²—ä½
        education_keywords = ["è¯¾ç¨‹", "é¡¾é—®", "æ•™å¸ˆ", "æ•™ç»ƒ", "æ‹›ç”Ÿ", "å­¦ç®¡", "ç­ä¸»ä»»", "æ•™ç ”"]
        is_education = any(k in job_title for k in education_keywords)
        
        if is_education:
            # æ•™è‚²è¡Œä¸šå²—ä½ï¼šåªå»é™¤æ˜æ˜¾çš„é”€å”®è¯æ±‡ï¼Œä¿ç•™æ‰€æœ‰å…¶ä»–å†…å®¹
            cleaned_evidence = []
            for ev in evidence_list:
                if ev and ev.strip():
                    # åªå»é™¤é”€å”®ç›¸å…³è¯æ±‡
                    cleaned = ev
                    for word in ["å¼€å‘å®¢æˆ·", "æ‹‰æ–°", "è½¬åŒ–", "é‚€çº¦", "ç”µé”€"]:
                        cleaned = cleaned.replace(word, "")
                    cleaned = re.sub(r"\s+", " ", cleaned).strip()
                    if cleaned:
                        cleaned_evidence.append(cleaned)
            
            # ç®€è¯„ä¹ŸåšåŒæ ·å¤„ç†
            summary_text = data.get("ç®€è¯„", "")
            if summary_text:
                for word in ["å¼€å‘å®¢æˆ·", "æ‹‰æ–°", "è½¬åŒ–", "é‚€çº¦", "ç”µé”€"]:
                    summary_text = summary_text.replace(word, "")
                summary_text = re.sub(r"\s+", " ", summary_text).strip()
            
            data["è¯æ®"] = cleaned_evidence
            data["ç®€è¯„"] = summary_text
        else:
            # éæ•™è‚²å²—ä½ï¼šè½»åº¦æ¸…æ´—ï¼Œä½†ä¿ç•™åŸå§‹å†…å®¹
            cleaned_evidence = []
            for ev in evidence_list:
                if ev and ev.strip():
                    cleaned = sanitize_ai_output(ev, job_title)
                    # å¦‚æœè¢«æ›¿æ¢ä¸ºå¼‚å¸¸æç¤ºï¼Œä¿ç•™åŸå§‹è¯æ®
                    if "å­˜åœ¨å¼‚å¸¸" in cleaned:
                        cleaned = ev  # å›é€€åˆ°åŸå§‹è¯æ®
                    if cleaned and cleaned.strip():
                        cleaned_evidence.append(cleaned)
            
            summary_text = data.get("ç®€è¯„", "")
            if summary_text:
                cleaned_summary = sanitize_ai_output(summary_text, job_title)
                # å¦‚æœè¢«æ›¿æ¢ä¸ºå¼‚å¸¸æç¤ºï¼Œä¿ç•™åŸå§‹ç®€è¯„
                if "å­˜åœ¨å¼‚å¸¸" in cleaned_summary:
                    cleaned_summary = summary_text
                summary_text = cleaned_summary
            
            data["è¯æ®"] = cleaned_evidence
            data["ç®€è¯„"] = summary_text

        applied_zero_fallback = False
        resume_length = len((resume_text or "").strip())
        if dimensions_all_zero and resume_length > 150:
            heuristic_scores = _heuristic_score_from_text(jd_text, resume_text, job_title)
            data["ç»´åº¦å¾—åˆ†"] = heuristic_scores["ç»´åº¦å¾—åˆ†"]
            data["æ€»åˆ†"] = heuristic_scores["æ€»åˆ†"]
            if not data.get("è¯æ®"):
                data["è¯æ®"] = heuristic_scores["è¯æ®"]
            if not data.get("ç®€è¯„"):
                data["ç®€è¯„"] = heuristic_scores["ç®€è¯„"]
            applied_zero_fallback = True

        try:
            ai_summary = _generate_short_eval(client, cfg, resume_text, job_title)
            # ç¡®ä¿ä¸æ˜¯å¼‚å¸¸æç¤ºï¼ˆä½¿ç”¨å®‰å…¨çš„å­—ç¬¦ä¸²æ£€æŸ¥ï¼‰
            try:
                ai_summary_str = _safe_str(ai_summary)
                if ai_summary_str and "å­˜åœ¨å¼‚å¸¸" in ai_summary_str:
                    # å¦‚æœè¢«æ›¿æ¢ä¸ºå¼‚å¸¸æç¤ºï¼Œä½¿ç”¨ç®€è¯„ä½œä¸ºæ›¿ä»£
                    ai_summary = data.get("ç®€è¯„", "è¯¥å€™é€‰äººå…·å¤‡ç›¸å…³å·¥ä½œç»éªŒï¼Œè¯·ç»“åˆç®€å†è¿›ä¸€æ­¥è¯„ä¼°ã€‚")
            except (UnicodeEncodeError, UnicodeError):
                # å¦‚æœæ£€æŸ¥æ—¶å‡ºç°ç¼–ç é”™è¯¯ï¼Œç›´æ¥ä½¿ç”¨ç®€è¯„
                ai_summary = data.get("ç®€è¯„", "è¯¥å€™é€‰äººå…·å¤‡ç›¸å…³å·¥ä½œç»éªŒï¼Œè¯·ç»“åˆç®€å†è¿›ä¸€æ­¥è¯„ä¼°ã€‚")
        except Exception as err:
            # API è°ƒç”¨å¤±è´¥æ—¶ï¼Œä½¿ç”¨ç®€è¯„æˆ–ç”Ÿæˆé€šç”¨è¯„ä»·
            try:
                err_str = _safe_str(err)[:50]
                ai_summary = data.get("ç®€è¯„", f"AIè¯„ä»·ç”Ÿæˆå¤±è´¥ï¼š{err_str}")
                data["çŸ­è¯„_error"] = err_str
            except (UnicodeEncodeError, UnicodeError):
                ai_summary = data.get("ç®€è¯„", "è¯¥å€™é€‰äººå…·å¤‡ç›¸å…³å·¥ä½œç»éªŒï¼Œè¯·ç»“åˆç®€å†è¿›ä¸€æ­¥è¯„ä¼°ã€‚")
                data["çŸ­è¯„_error"] = "ç¼–ç é”™è¯¯"
        try:
            if applied_zero_fallback:
                data["short_eval"] = f"[AI åˆå§‹è¯„åˆ†ä¸º 0ï¼Œå·²å›é€€å¯å‘å¼] {ai_summary}"
            else:
                data["short_eval"] = ai_summary
        except (UnicodeEncodeError, UnicodeError):
            # å¦‚æœèµ‹å€¼æ—¶å‡ºç°ç¼–ç é”™è¯¯ï¼Œä½¿ç”¨å®‰å…¨çš„é»˜è®¤å€¼
            data["short_eval"] = "è¯¥å€™é€‰äººå…·å¤‡ç›¸å…³å·¥ä½œç»éªŒï¼Œè¯·ç»“åˆç®€å†è¿›ä¸€æ­¥è¯„ä¼°ã€‚"
        
        # ğŸ”§ æœ€ç»ˆç»Ÿä¸€æ›¿æ¢ï¼šç¡®ä¿æ‰€æœ‰å­—æ®µéƒ½ä¸åŒ…å«å¼‚å¸¸æç¤º
        fallback_text = "è¯¥å€™é€‰äººå…·å¤‡ç›¸å…³å·¥ä½œç»éªŒï¼Œè¯·ç»“åˆç®€å†è¿›ä¸€æ­¥è¯„ä¼°ã€‚"
        
        # æ›¿æ¢è¯æ®ä¸­çš„å¼‚å¸¸æç¤ºï¼ˆä½¿ç”¨å®‰å…¨çš„å­—ç¬¦ä¸²å¤„ç†ï¼‰
        try:
            evidence_list = data.get("è¯æ®", [])
            cleaned_evidence = []
            for ev in evidence_list:
                try:
                    # å®‰å…¨åœ°è½¬æ¢å’Œæ£€æŸ¥å­—ç¬¦ä¸²
                    ev_str = _safe_str(ev)
                    # å®‰å…¨åœ°æ£€æŸ¥å­—ç¬¦ä¸²ï¼Œé¿å…ç¼–ç é”™è¯¯
                    if ev_str:
                        try:
                            if "å­˜åœ¨å¼‚å¸¸" not in ev_str:
                                cleaned_evidence.append(ev)
                        except (UnicodeEncodeError, UnicodeError):
                            # å¦‚æœæ£€æŸ¥æ—¶å‡ºé”™ï¼Œè·³è¿‡è¿™ä¸ªè¯æ®
                            continue
                except (UnicodeEncodeError, UnicodeError, Exception):
                    # å¦‚æœå¤„ç†å•ä¸ªè¯æ®æ—¶å‡ºé”™ï¼Œè·³è¿‡å®ƒ
                    continue
            if not cleaned_evidence and evidence_list:
                # å¦‚æœæ‰€æœ‰è¯æ®éƒ½è¢«è¿‡æ»¤ï¼Œè‡³å°‘ä¿ç•™ä¸€æ¡é€šç”¨æè¿°
                cleaned_evidence = ["å€™é€‰äººå…·å¤‡ç›¸å…³å·¥ä½œç»éªŒã€‚"]
            data["è¯æ®"] = cleaned_evidence
        except (UnicodeEncodeError, UnicodeError, Exception) as e:
            # å¦‚æœå¤„ç†è¯æ®æ—¶å‡ºé”™ï¼Œä½¿ç”¨é»˜è®¤å€¼
            data["è¯æ®"] = ["å€™é€‰äººå…·å¤‡ç›¸å…³å·¥ä½œç»éªŒã€‚"]
        
        # æ›¿æ¢ç®€è¯„ä¸­çš„å¼‚å¸¸æç¤ºï¼ˆä½¿ç”¨å®‰å…¨çš„å­—ç¬¦ä¸²å¤„ç†ï¼‰
        try:
            if "ç®€è¯„" in data:
                summary = data["ç®€è¯„"]
                if summary:
                    try:
                        # å®‰å…¨åœ°è½¬æ¢å­—ç¬¦ä¸²
                        summary_str = _safe_str(summary)
                        # å®‰å…¨åœ°æ£€æŸ¥å­—ç¬¦ä¸²
                        try:
                            if summary_str and "å­˜åœ¨å¼‚å¸¸" in summary_str:
                                data["ç®€è¯„"] = fallback_text
                        except (UnicodeEncodeError, UnicodeError):
                            data["ç®€è¯„"] = fallback_text
                    except (UnicodeEncodeError, UnicodeError):
                        data["ç®€è¯„"] = fallback_text
        except (UnicodeEncodeError, UnicodeError, Exception):
            if "ç®€è¯„" in data:
                data["ç®€è¯„"] = fallback_text
        
        # æ›¿æ¢ short_eval ä¸­çš„å¼‚å¸¸æç¤ºï¼ˆä½¿ç”¨å®‰å…¨çš„å­—ç¬¦ä¸²å¤„ç†ï¼‰
        try:
            if "short_eval" in data:
                short_eval = data["short_eval"]
                if short_eval:
                    try:
                        # å®‰å…¨åœ°è½¬æ¢å­—ç¬¦ä¸²
                        short_eval_str = _safe_str(short_eval)
                        # å®‰å…¨åœ°æ£€æŸ¥å­—ç¬¦ä¸²
                        try:
                            if short_eval_str and "å­˜åœ¨å¼‚å¸¸" in short_eval_str:
                                data["short_eval"] = fallback_text
                        except (UnicodeEncodeError, UnicodeError):
                            data["short_eval"] = fallback_text
                    except (UnicodeEncodeError, UnicodeError):
                        data["short_eval"] = fallback_text
        except (UnicodeEncodeError, UnicodeError, Exception):
            if "short_eval" in data:
                data["short_eval"] = fallback_text
        
        return data
    except (UnicodeEncodeError, UnicodeError) as e:
        # å¦‚æœæ•´ä¸ªå‡½æ•°æ‰§è¡Œè¿‡ç¨‹ä¸­å‡ºç°ç¼–ç é”™è¯¯ï¼Œè¿”å›å®‰å…¨çš„é»˜è®¤å€¼
        return {
            "æ€»åˆ†": 0,
            "ç»´åº¦å¾—åˆ†": {"æŠ€èƒ½åŒ¹é…åº¦": 0, "ç»éªŒç›¸å…³æ€§": 0, "æˆé•¿æ½œåŠ›": 0, "ç¨³å®šæ€§": 0},
            "è¯æ®": ["å€™é€‰äººå…·å¤‡ç›¸å…³å·¥ä½œç»éªŒã€‚"],
            "ç®€è¯„": "è¯¥å€™é€‰äººå…·å¤‡ç›¸å…³å·¥ä½œç»éªŒï¼Œè¯·ç»“åˆç®€å†è¿›ä¸€æ­¥è¯„ä¼°ã€‚",
            "short_eval": "è¯¥å€™é€‰äººå…·å¤‡ç›¸å…³å·¥ä½œç»éªŒï¼Œè¯·ç»“åˆç®€å†è¿›ä¸€æ­¥è¯„ä¼°ã€‚",
            "ç¼–ç é”™è¯¯": "å¤„ç†è¿‡ç¨‹ä¸­å‡ºç°ç¼–ç é—®é¢˜ï¼Œå·²ä½¿ç”¨é»˜è®¤å€¼"
        }
    except Exception as e:
        # å…¶ä»–å¼‚å¸¸ï¼šä½¿ç”¨å¯å‘å¼è¯„åˆ†å…œåº•ï¼Œè€Œä¸æ˜¯å…¨ 0 åˆ†
        fallback = _heuristic_score_from_text(jd_text, resume_text, job_title)
        fallback["å¤„ç†é”™è¯¯"] = _safe_str(e)[:100]
        return fallback


def ai_match_resumes_df(jd_text: str, resumes_df: pd.DataFrame, job_title: str = "") -> pd.DataFrame:
    """
    å¯¹å¤–ç»Ÿä¸€å…¥å£ï¼šåŸºäº AI æ‰“åˆ†ï¼Œå¤±è´¥æ—¶è‡ªåŠ¨å›é€€åˆ°å¯å‘å¼è¯„åˆ†ï¼Œé¿å…â€œå…¨ 0 åˆ†â€ã€‚
    """
    # åœ¨å‡½æ•°å¼€å§‹æ—¶è®¾ç½® stdout ç¼–ç ï¼Œé¿å…åç»­ç¼–ç é”™è¯¯
    try:
        import sys
        import io
        if sys.stdout.encoding and sys.stdout.encoding.lower() != 'utf-8':
            if not hasattr(sys.stdout, '_original_write'):
                sys.stdout._original_write = sys.stdout.write

                def safe_write(s):
                    try:
                        sys.stdout._original_write(s)
                    except UnicodeEncodeError:
                        safe_s = s.encode('utf-8', errors='replace').decode('utf-8', errors='replace')
                        sys.stdout._original_write(safe_s)

                sys.stdout.write = safe_write
    except Exception:
        pass

    try:
        client, cfg = get_client_and_cfg()
        ai_available = True
    except Exception as err:
        ai_available = False
        _safe_print(f"[AI matcher] è·å– AI å®¢æˆ·ç«¯å¤±è´¥ï¼Œå°†ä½¿ç”¨å¯å‘å¼è¯„åˆ†ï¼š{err}")
        return _heuristic_match_resumes_df(jd_text, resumes_df, job_title)

    if not job_title:
        job_title = "é”€å”®é¡¾é—®"

    try:
        job_family = infer_job_family(job_title)
        strategy_category, _ = determine_competency_strategy(job_title)
    except Exception:
        job_family = "generic"
        strategy_category = "é€šç”¨ç»´åº¦"

    if strategy_category and strategy_category != "é€šç”¨ç»´åº¦":
        effective_job_label = strategy_category
    elif job_family and job_family != "general":
        effective_job_label = job_family
    else:
        effective_job_label = job_title

    if "resume_text" not in resumes_df.columns:
        resumes_df = resumes_df.copy()
        fallback_candidates = ["text", "full_text", "content", "parsed_text"]
        fallback = next((col for col in fallback_candidates if col in resumes_df.columns), None)
        if fallback:
            resumes_df["resume_text"] = resumes_df[fallback].fillna("")
        else:
            resumes_df["resume_text"] = ""

    rows = []
    for idx in resumes_df.index:
        resume_text = _safe_str(resumes_df.loc[idx, "resume_text"] or "")
        file_name = resumes_df.loc[idx, "file"] if "file" in resumes_df.columns else ""

        if ai_available:
            try:
                result = ai_score_one(client, cfg, jd_text, resume_text, effective_job_label)
            except Exception as e:
                # å¦‚æœå•æ¡ AI è°ƒç”¨å¤±è´¥ï¼Œå›é€€åˆ°å¯å‘å¼è¯„åˆ†
                result = _heuristic_score_from_text(jd_text, resume_text, effective_job_label)
                result["short_eval"] = result.get("short_eval") or f"AIæ™ºèƒ½è¯„ä»·å¤±è´¥ï¼š{_safe_str(e)}"
        else:
            result = _heuristic_score_from_text(jd_text, resume_text, effective_job_label)

        rows.append(
            {
                "candidate_id": resumes_df.loc[idx, "candidate_id"] if "candidate_id" in resumes_df.columns else None,
                "file": file_name,
                "name": resumes_df.loc[idx, "name"] if "name" in resumes_df.columns else "",
                "email": resumes_df.loc[idx, "email"] if "email" in resumes_df.columns else "",
                "phone": resumes_df.loc[idx, "phone"] if "phone" in resumes_df.columns else "",
                "resume_text": resume_text,
                "æ€»åˆ†": result.get("æ€»åˆ†", 0),
                "æŠ€èƒ½åŒ¹é…åº¦": result.get("ç»´åº¦å¾—åˆ†", {}).get("æŠ€èƒ½åŒ¹é…åº¦", 0),
                "ç»éªŒç›¸å…³æ€§": result.get("ç»´åº¦å¾—åˆ†", {}).get("ç»éªŒç›¸å…³æ€§", 0),
                "æˆé•¿æ½œåŠ›": result.get("ç»´åº¦å¾—åˆ†", {}).get("æˆé•¿æ½œåŠ›", 0),
                "ç¨³å®šæ€§": result.get("ç»´åº¦å¾—åˆ†", {}).get("ç¨³å®šæ€§", 0),
                "short_eval": result.get("short_eval") or result.get("ç®€è¯„", ""),
                "è¯æ®": _safe_join(result.get("è¯æ®") or [], "ï¼›"),
                "text_len": resumes_df.loc[idx, "text_len"] if "text_len" in resumes_df.columns else len(resume_text),
            }
        )

    df = pd.DataFrame(rows)

    if "ç®€è¯„" in df.columns and "short_eval" not in df.columns:
        df["short_eval"] = df.pop("ç®€è¯„")

    return df



